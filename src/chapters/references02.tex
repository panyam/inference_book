% =============================================================================
% References for Chapter 2: How LLMs Work
% =============================================================================

\begin{thebibliography}{99}

\bibitem{vaswani2017attention}
Vaswani, A., et al.: Attention Is All You Need. In: Advances in Neural Information Processing Systems (NeurIPS) 2017. \url{https://arxiv.org/abs/1706.03762}

\bibitem{radford2018gpt}
Radford, A., et al.: Improving Language Understanding by Generative Pre-Training. OpenAI (2018). \url{https://openai.com/research/language-unsupervised}

\bibitem{jiang2024mixtral}
Jiang, A., et al.: Mixtral of Experts. Mistral AI (2024). \url{https://arxiv.org/abs/2401.04088}

\bibitem{gu2023mamba}
Gu, A., Dao, T.: Mamba: Linear-Time Sequence Modeling with Selective State Spaces. (2023). \url{https://arxiv.org/abs/2312.00752}

\bibitem{pope2022efficiently}
Pope, R., et al.: Efficiently Scaling Transformer Inference. MLSys 2023. \url{https://arxiv.org/abs/2211.05102}

\end{thebibliography}
