%%%%%%%%%%%%%%%%%%%%% chapter04.tex %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% Chapter 4: Model Formats and Quantization
%
%%%%%%%%%%%%%%%%%%%%%%%% Springer-Verlag %%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Model Formats and Quantization}
\label{ch:models}

\abstract*{The choice of model format and quantization level directly impacts performance, quality, and resource requirements. This chapter explains the major model formats (SafeTensors, GGUF, ONNX), demystifies quantization techniques (GPTQ, AWQ, GGML/GGUF), provides guidance on choosing models for different use cases, and introduces the concept of a model registry for your control plane. We focus on the practical knowledge needed to make informed model selection decisions.}

\abstract{The choice of model format and quantization level directly impacts performance, quality, and resource requirements. This chapter explains the major model formats (SafeTensors, GGUF, ONNX), demystifies quantization techniques (GPTQ, AWQ, GGML/GGUF), provides guidance on choosing models for different use cases, and introduces the concept of a model registry for your control plane. We focus on the practical knowledge needed to make informed model selection decisions.}

% =============================================================================
\section{Understanding Model Formats}
\label{sec:model-formats}
% =============================================================================

Chapter~\ref{ch:hardware-fundamentals} showed us how to calculate VRAM requirements: a 7B model at FP16 needs 14 GB, but quantized to INT4 needs only 3.5 GB. But where do these quantized models come from? How do you choose between the dozens of variants on Hugging Face? This chapter answers these questions.

The path from a trained model to inference involves several decisions: which format to use (SafeTensors, GGUF, ONNX), which quantization method (GPTQ, AWQ, GGUF's built-in quantization), and which specific quantization level (Q4, Q5, Q8). Each choice affects memory usage, inference speed, and output quality. By the end of this chapter, you'll know how to navigate these options and select the right model for your hardware and use case.

\subsection{SafeTensors}
\label{subsec:safetensors}

% TODO: The modern standard
% - What it is
% - Safety advantages over pickle
% - Where it's used (HuggingFace standard)
% - Loading and inspection

\subsection{GGUF (GPT-Generated Unified Format)}
\label{subsec:gguf}

% TODO: The llama.cpp format
% - History: GGML -> GGUF
% - Single-file advantage
% - Metadata embedding
% - Quantization built-in
% - Where to find GGUF models

\subsection{ONNX (Open Neural Network Exchange)}
\label{subsec:onnx}

% TODO: Cross-platform format
% - Purpose and advantages
% - When to use
% - Conversion considerations

\subsection{Format Comparison}
\label{subsec:format-comparison}

% TODO: Comparison table
% | Format | Use Case | Quantization | Ecosystem |
% | SafeTensors | Training, HF | External | PyTorch, TF |
% | GGUF | Inference | Built-in | llama.cpp |
% | ONNX | Cross-platform | External | Multiple |

% =============================================================================
\section{Quantization Fundamentals}
\label{sec:quantization-fundamentals}
% =============================================================================

% TODO: What quantization is and why it matters

\subsection{What is Quantization?}
\label{subsec:what-is-quantization}

% TODO: Core concepts
% - Reducing precision
% - Memory savings
% - Speed improvements
% - Quality trade-offs

\subsection{Quantization Levels}
\label{subsec:quantization-levels}

% TODO: Common quantization levels
% - Q8 (8-bit)
% - Q6 (6-bit)
% - Q5 (5-bit)
% - Q4 (4-bit) - sweet spot
% - Q3/Q2 (aggressive)

\begin{svgraybox}
\textbf{The Quantization Sweet Spot:}

For most use cases, Q4 (4-bit) quantization provides the best balance:
\begin{itemize}
\item 4x memory reduction vs FP16
\item Minimal quality degradation (typically <3\%)
\item Sufficient precision for most tasks
\item Good inference speed
\end{itemize}

Go lower (Q3/Q2) only when memory-constrained. Go higher (Q5/Q6) for quality-critical applications.
\end{svgraybox}

% =============================================================================
\section{Quantization Methods}
\label{sec:quantization-methods}
% =============================================================================

% TODO: Different quantization approaches

\subsection{GGUF Quantization (llama.cpp)}
\label{subsec:gguf-quantization}

% TODO: GGUF quantization types
% - Q4_K_M, Q4_K_S explained
% - Q5_K_M, Q5_K_S
% - What the K, M, S mean
% - Choosing the right variant

\subsection{GPTQ}
\label{subsec:gptq}

% TODO: GPTQ quantization
% - How it works (calibration-based)
% - Advantages: accuracy preservation
% - Disadvantages: requires GPU inference
% - When to use

\subsection{AWQ (Activation-aware Weight Quantization)}
\label{subsec:awq}

% TODO: AWQ quantization
% - How it differs from GPTQ
% - Performance characteristics
% - Ecosystem support

\subsection{BitsAndBytes}
\label{subsec:bitsandbytes}

% TODO: HuggingFace integration
% - 8-bit and 4-bit options
% - nf4 quantization
% - When to use

\subsection{Method Comparison}
\label{subsec:quant-method-comparison}

% TODO: When to use each
% | Method | Ecosystem | CPU Support | GPU Support | Quality |
% | GGUF | llama.cpp | Yes | Yes | Good |
% | GPTQ | HF/vLLM | No | Yes | Excellent |
% | AWQ | vLLM | No | Yes | Excellent |
% | BnB | HF | Limited | Yes | Good |

% =============================================================================
\section{Choosing Models for Your Use Case}
\label{sec:choosing-models}
% =============================================================================

% TODO: Decision framework

\subsection{Model Families Overview}
\label{subsec:model-families}

% TODO: Current landscape
% - Llama family (Meta)
% - Mistral family
% - Qwen family (Alibaba)
% - Phi family (Microsoft)
% - Gemma family (Google)
% - Open-source vs open-weight

\subsection{Size vs Quality Trade-offs}
\label{subsec:size-quality}

% TODO: How to think about model size
% - Benchmarks and their limitations
% - Real-world testing importance
% - Task-specific performance

\subsection{Decision Matrix}
\label{subsec:model-decision-matrix}

% TODO: Practical recommendations
% | Task | Min Size | Recommended | Notes |
% | Simple chat | 1B | 7B | TinyLlama, Phi-3 |
% | Code generation | 7B | 13B-33B | CodeLlama, Qwen-Coder |
% | Complex reasoning | 13B | 70B | Llama 3.1, Mistral |
% | Production | 7B-13B | 30B | Balance quality/cost |

% =============================================================================
\section{Where to Find Models}
\label{sec:finding-models}
% =============================================================================

% TODO: Model sources

\subsection{Hugging Face Hub}
\label{subsec:huggingface-hub}

% TODO: The main repository
% - Navigation
% - Finding quantized versions
% - Evaluating model cards
% - TheBloke's quantizations

\subsection{Ollama Library}
\label{subsec:ollama-library}

% TODO: Ollama's curated collection
% - Pre-configured models
% - Version management
% - Custom modelfiles

\subsection{Model Evaluation}
\label{subsec:model-evaluation}

% TODO: How to evaluate models
% - Benchmark scores (limitations)
% - Your own test suite
% - A/B testing

% =============================================================================
\section{Building a Model Registry}
\label{sec:model-registry}
% =============================================================================

% TODO: Introduce the concept for control plane

\subsection{Why a Registry?}
\label{subsec:why-registry}

% TODO: Benefits
% - Centralized metadata
% - Version tracking
% - Capability mapping
% - Cost tracking

\subsection{Registry Data Model}
\label{subsec:registry-data-model}

\begin{programcode}{Model Registry Interface (Go)}
\begin{lstlisting}[language=Go]
// ModelInfo represents metadata about a model
type ModelInfo struct {
    ID            string            `json:"id"`
    Name          string            `json:"name"`
    Family        string            `json:"family"`
    Parameters    string            `json:"parameters"` // e.g., "7B"
    Quantization  string            `json:"quantization"` // e.g., "Q4_K_M"
    Format        string            `json:"format"` // gguf, safetensors
    VRAMRequired  int64             `json:"vram_required_mb"`
    ContextWindow int               `json:"context_window"`
    Capabilities  []string          `json:"capabilities"`
    CostPerToken  float64           `json:"cost_per_token"`
    Metadata      map[string]string `json:"metadata"`
}

// ModelRegistry manages available models
type ModelRegistry interface {
    List(ctx context.Context) ([]ModelInfo, error)
    Get(ctx context.Context, modelID string) (ModelInfo, error)
    Register(ctx context.Context, model ModelInfo) error
    FindByCapability(ctx context.Context, cap string) ([]ModelInfo, error)
}
\end{lstlisting}
\end{programcode}

% =============================================================================
\section{Summary}
\label{sec:ch04-summary}
% =============================================================================

This chapter covered the practical decisions involved in selecting and preparing models for inference. Model formats determine compatibility with inference engines---GGUF for llama.cpp's flexibility, SafeTensors for the Hugging Face ecosystem, and ONNX for cross-platform deployment. Quantization methods trade quality for memory, with Q4 providing an excellent balance for most use cases.

The model registry concept introduced here becomes part of your control plane, tracking which models are available, their resource requirements, and their capabilities. This metadata drives routing decisions when you support multiple models.

With models selected and ready, Chapter~\ref{ch:inference-engines} introduces the software that actually runs them: inference engines like Ollama, llama.cpp, and vLLM. Each engine has different strengths---simplicity, flexibility, or production performance---and choosing the right one depends on your deployment scenario.

\begin{important}{Key Takeaways}
\begin{itemize}
\item GGUF is the format of choice for flexible inference (CPU + GPU support)
\item Q4 quantization provides the best quality/memory trade-off for most cases
\item GPTQ/AWQ offer superior quality but require GPU-only inference
\item Model choice depends on task complexity, resource constraints, and quality needs
\item A model registry helps manage the growing ecosystem of options
\end{itemize}
\end{important}

% =============================================================================
\section*{Problems}
\addcontentsline{toc}{section}{Problems}
% =============================================================================

\begin{prob}
\label{prob:ch03-quantization-comparison}
\textbf{Quantization Quality Comparison}\\
Download the same base model in Q4, Q5, and Q8 quantization. Using a consistent set of 20 prompts (10 factual, 10 creative), evaluate the output quality. Document your methodology and findings.
\end{prob}

\begin{prob}
\label{prob:ch03-model-selection}
\textbf{Model Selection for Use Case}\\
You're building a customer support chatbot that needs to handle billing questions, technical troubleshooting, and appointment scheduling. You have 16GB VRAM available. Research and recommend a specific model, justifying your choice.
\end{prob}

\begin{prob}
\label{prob:ch03-registry-implementation}
\textbf{Model Registry Implementation}\\
Implement the \texttt{ModelRegistry} interface using a JSON file as the backing store. Include methods for adding models, listing by capability, and finding the best model for a given VRAM constraint.
\end{prob}

\input{chapters/references04}
