%%%%%%%%%%%%%%%%%%%%% chapter02.tex %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% Chapter 2: Hardware Fundamentals
%
%%%%%%%%%%%%%%%%%%%%%%%% Springer-Verlag %%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Hardware Fundamentals}
\label{ch:hardware}

\abstract*{Understanding hardware is essential for making informed decisions about self-hosted inference. This chapter covers how transformers use memory during inference, VRAM calculation formulas, CPU vs GPU trade-offs, and a comprehensive comparison of available hardware options. We also analyze the economics of owned versus rented infrastructure to help you make the right choice for your situation.}

\abstract{Understanding hardware is essential for making informed decisions about self-hosted inference. This chapter covers how transformers use memory during inference, VRAM calculation formulas, CPU vs GPU trade-offs, and a comprehensive comparison of available hardware options. We also analyze the economics of owned versus rented infrastructure to help you make the right choice for your situation.}

% =============================================================================
\section{How Transformers Use Memory}
\label{sec:transformer-memory}
% =============================================================================

Before selecting hardware, you need to understand what consumes memory during inference. Three components dominate: model weights, the KV cache, and working memory for activations. Each scales differently with model size and usage patterns.

\subsection{Model Weights}
\label{subsec:model-weights}

Model weights are the learned parameters---the numbers that make a model behave intelligently. A "7B model" has approximately 7 billion parameters. Each parameter requires storage, and the memory needed depends on numerical precision.

The formula is straightforward:

\begin{equation}
\text{Weight Memory} = \text{Parameters} \times \text{Bytes per Parameter}
\end{equation}

At full precision (FP32), each parameter uses 4 bytes. A 7B model requires $7 \times 10^9 \times 4 = 28$ GB just for weights---more than most consumer GPUs offer. This is why nobody runs large models at FP32 for inference.

Half precision (FP16 or BF16) cuts this in half: 2 bytes per parameter, so 14 GB for a 7B model. This fits on an RTX 4090 (24 GB) with room to spare, but exceeds an RTX 4070 Ti (16 GB) when you account for other memory needs.

Quantization reduces precision further. At 4-bit precision (INT4), each parameter uses only 0.5 bytes, bringing our 7B model down to 3.5 GB. This is why quantized models run on laptops and modest GPUs---the memory reduction is dramatic:

\begin{table}[htbp]
\centering
\caption{Weight memory for common model sizes at different precisions.}
\label{tab:weight-memory}
\begin{tabular}{lrrrr}
\toprule
\textbf{Model} & \textbf{FP32} & \textbf{FP16} & \textbf{INT8} & \textbf{INT4} \\
\midrule
7B   & 28 GB  & 14 GB  & 7 GB   & 3.5 GB \\
13B  & 52 GB  & 26 GB  & 13 GB  & 6.5 GB \\
30B  & 120 GB & 60 GB  & 30 GB  & 15 GB \\
70B  & 280 GB & 140 GB & 70 GB  & 35 GB \\
400B & 1.6 TB & 800 GB & 400 GB & 200 GB \\
\bottomrule
\end{tabular}
\end{table}

These numbers represent minimums---the weights alone, with nothing else. Actual VRAM usage is higher due to the KV cache and working memory we'll cover next.

\subsection{The KV Cache}
\label{subsec:kv-cache}

The KV cache is the second major memory consumer, and unlike weights, it grows during inference. Understanding it explains why long conversations consume more memory than short ones.

During autoregressive generation, the model generates one token at a time. For each new token, it must "attend" to all previous tokens---the attention mechanism needs to compute relationships between the new token and everything that came before. Without caching, this would require reprocessing the entire context for every generated token, making generation quadratically slower.

The KV cache stores the key and value vectors computed during attention. Once computed, these don't change, so caching them avoids redundant computation. The trade-off: memory consumption that grows linearly with context length.

The KV cache size depends on model architecture:

\begin{equation}
\text{KV Cache} = 2 \times L \times H \times D \times C \times B \times N
\end{equation}

Where:
\begin{itemize}
\item $L$ = number of layers
\item $H$ = number of attention heads (or KV heads for GQA)
\item $D$ = dimension per head
\item $C$ = context length (tokens)
\item $B$ = bytes per value (2 for FP16)
\item $N$ = batch size
\item Factor of 2 accounts for both keys and values
\end{itemize}

For a typical 7B model (32 layers, 32 heads, 128 dimensions per head) at FP16:

\begin{equation}
\text{KV Cache} = 2 \times 32 \times 32 \times 128 \times C \times 2 = 524,288 \times C \text{ bytes}
\end{equation}

At 4096 context tokens: $\sim$2 GB. At 32K context: $\sim$16 GB. At 128K context: $\sim$64 GB---potentially exceeding the weight memory itself.

This is why long-context inference is memory-intensive. A 7B model that fits comfortably in 8 GB VRAM at 4K context may require 24 GB at 32K context. Grouped Query Attention (GQA), used in newer models like Llama 3 and Qwen 2.5, reduces KV cache size by sharing key-value heads, typically by 4--8x.

\subsection{Activation and Working Memory}
\label{subsec:activation-memory}

Beyond weights and KV cache, inference requires working memory for intermediate computations. This includes attention scores, feed-forward network activations, and temporary buffers for matrix operations.

For a single forward pass, activation memory scales with model dimensions and batch size:

\begin{equation}
\text{Activation Memory} \approx B \times C \times D_{\text{model}} \times L \times k
\end{equation}

Where $k$ is a small constant (typically 2--4) depending on the engine's memory management. For a 7B model with batch size 1 and 4K context, activations consume roughly 200--500 MB.

This seems small compared to weights and KV cache, but it matters for two reasons. First, activation memory must be allocated contiguously, and memory fragmentation can prevent allocation even when total free memory appears sufficient. Second, during batched inference, activation memory grows linearly with batch size.

\subsubsection*{The Overhead Budget}

Practical VRAM planning requires a safety margin. CUDA maintains its own memory pools, the inference engine has overhead, and memory fragmentation accumulates over time. A conservative estimate adds 10--20\% to your calculated requirements:

\begin{equation}
\text{Total VRAM} \approx (\text{Weights} + \text{KV Cache} + \text{Activations}) \times 1.15
\end{equation}

If your calculations show 20 GB needed, plan for 23 GB to be safe. This matters when choosing between a 24 GB GPU that barely fits and a 48 GB GPU with comfortable headroom. The latter allows longer contexts, larger batches, or simply more reliable operation.

% =============================================================================
\section{VRAM Calculation}
\label{sec:vram-calculation}
% =============================================================================

% TODO: The master formula

\begin{svgraybox}
\textbf{VRAM Estimation Formula:}

For a model with $P$ parameters at precision $B$ bytes:

\begin{equation}
\text{VRAM}_{\text{base}} = P \times B
\end{equation}

For inference with context length $C$ and batch size $N$:

\begin{equation}
\text{VRAM}_{\text{total}} \approx P \times B + \text{KV\_cache}(C, N) + \text{overhead}
\end{equation}

Where overhead is typically 10--20\% additional memory.
\end{svgraybox}

\subsection{Precision and Memory}
\label{subsec:precision}

% TODO: Table of precision formats
% | Precision | Bytes | Use Case |
% | FP32 | 4 | Training (rarely inference) |
% | FP16 | 2 | Standard GPU inference |
% | BF16 | 2 | Standard GPU inference |
% | INT8 | 1 | Quantized inference |
% | INT4 | 0.5 | Aggressive quantization |

\subsection{Worked Examples}
\label{subsec:vram-examples}

% TODO: Calculate for our target models
% - 7B model at different precisions
% - 30B model requirements
% - 70B model requirements
% - 400B model requirements

% =============================================================================
\section{CPU vs GPU Inference}
\label{sec:cpu-vs-gpu}
% =============================================================================

% TODO: When each makes sense

\subsection{CPU Inference}
\label{subsec:cpu-inference}

% TODO: CPU characteristics
% - Larger memory (RAM vs VRAM)
% - Lower throughput
% - llama.cpp optimization
% - When it works well

\subsection{GPU Inference}
\label{subsec:gpu-inference}

% TODO: GPU advantages
% - Parallelism
% - Memory bandwidth
% - Specialized tensor cores
% - When it's required

\subsection{Hybrid Approaches}
\label{subsec:hybrid-cpu-gpu}

% TODO: CPU+GPU together
% - Offloading layers
% - llama.cpp layer splitting
% - Trade-offs

% =============================================================================
\section{GPU Hardware Comparison}
\label{sec:gpu-comparison}
% =============================================================================

% TODO: Comprehensive GPU comparison

\subsection{NVIDIA Consumer GPUs}
\label{subsec:nvidia-consumer}

% TODO: RTX series comparison
% - RTX 3060 12GB
% - RTX 3090 24GB
% - RTX 4060 8GB
% - RTX 4070 Ti 16GB
% - RTX 4080 16GB
% - RTX 4090 24GB

\subsection{NVIDIA Professional GPUs}
\label{subsec:nvidia-professional}

% TODO: Workstation and datacenter
% - RTX A4000/A5000/A6000
% - A100 40GB/80GB
% - H100 80GB
% - Comparison table

\subsection{AMD GPUs}
\label{subsec:amd-gpus}

% TODO: AMD options
% - ROCm support status
% - RX 7900 XTX
% - Instinct MI series
% - Trade-offs vs NVIDIA

\subsection{Apple Silicon}
\label{subsec:apple-silicon}

% TODO: M-series chips
% - Unified memory advantage
% - M1/M2/M3 performance
% - M1/M2/M3 Pro/Max/Ultra
% - When they make sense

\subsection{Hardware Recommendation Matrix}
\label{subsec:hardware-matrix}

% TODO: Decision matrix table
% | Model Size | Min VRAM | Recommended | Budget Option |
% | 7B Q4 | 4GB | RTX 4060 8GB | M1 MacBook |
% | 7B FP16 | 14GB | RTX 4090 24GB | RTX 3090 24GB |
% | etc. |

% =============================================================================
\section{Owned vs Rented Infrastructure}
\label{sec:owned-vs-rented}
% =============================================================================

% TODO: The big economic question

\subsection{Buying Hardware}
\label{subsec:buying-hardware}

% TODO: Ownership economics
% - Upfront CapEx
% - Ongoing costs (power, cooling, maintenance)
% - Depreciation
% - Tax implications
% - When it makes sense

\subsection{Renting Cloud GPUs}
\label{subsec:renting-gpus}

% TODO: Rental options
% - RunPod
% - Vast.ai
% - Lambda Labs
% - Major cloud providers (AWS, GCP, Azure)
% - Pricing comparison

\subsection{Break-Even Analysis}
\label{subsec:break-even}

% TODO: When to buy vs rent
% - Formula for break-even
% - Worked examples
% - Utilization factor
% - Decision framework

\begin{svgraybox}
\textbf{Break-Even Formula:}

\begin{equation}
\text{Break-even (months)} = \frac{\text{Hardware Cost} + \text{Setup Costs}}{\text{Monthly Rental Cost} - \text{Monthly Operating Cost}}
\end{equation}

Where Monthly Operating Cost includes power, cooling, and amortized maintenance.
\end{svgraybox}

\subsection{Hybrid Strategies}
\label{subsec:hybrid-strategies}

% TODO: Best of both worlds
% - Own base capacity
% - Rent for burst
% - Geographic distribution

% =============================================================================
\section{Power and Cooling Considerations}
\label{sec:power-cooling}
% =============================================================================

% TODO: Practical infrastructure
% - Power consumption by GPU
% - Cooling requirements
% - Home lab considerations
% - Data center placement

% =============================================================================
\section{TPU as an Alternative}
\label{sec:tpu-sidebar}
% =============================================================================

\begin{backgroundinformation}{Google Cloud TPUs}
While this book primarily focuses on GPU-based inference, Google Cloud TPUs offer an alternative worth considering. TPUs can be 50--70\% cheaper than equivalent GPUs for sustained workloads, but require JAX-based models and are only available on Google Cloud Platform.

If you're deploying on GCP with JAX models, see Appendix~\ref{ch:appendix-tpu} for complete coverage of TPU inference.
\end{backgroundinformation}

% =============================================================================
\section{Summary}
\label{sec:ch02-summary}
% =============================================================================

% TODO: Chapter summary

\begin{important}{Key Takeaways}
\begin{itemize}
\item VRAM is the primary constraint for GPU inference
\item Quantization dramatically reduces memory requirements (4x for INT4)
\item CPU inference is viable for smaller models with enough RAM
\item Hardware choice depends on model size, budget, and utilization
\item Break-even between owned and rented depends on consistent usage
\end{itemize}
\end{important}

% =============================================================================
\section*{Problems}
\addcontentsline{toc}{section}{Problems}
% =============================================================================

\begin{prob}
\label{prob:ch02-vram-calc}
\textbf{VRAM Calculation}\\
Calculate the minimum VRAM required to run a 30B parameter model at FP16 precision with a context length of 4096 tokens. Then calculate the VRAM needed if quantized to INT4.
\end{prob}

\begin{prob}
\label{prob:ch02-break-even}
\textbf{Break-Even Analysis}\\
You need to serve a 7B model 24/7. Calculate the break-even point (in months) between buying an RTX 4070 Ti (\$800) versus renting an equivalent GPU on RunPod (\$0.30/hour). Assume \$0.15/kWh electricity and 200W GPU power consumption.
\end{prob}

\begin{prob}
\label{prob:ch02-hardware-selection}
\textbf{Hardware Selection}\\
A startup needs to serve 1000 requests/hour with a 7B model. They have a budget of \$5,000. Recommend a hardware configuration and justify your choice, considering both owned and rented options.
\end{prob}

\input{chapters/references02}
