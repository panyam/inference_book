% =============================================================================
% Chapter 2: How LLMs Work
% =============================================================================
% This chapter provides the conceptual foundation for understanding hardware
% requirements. Readers will understand WHY transformers need memory before
% Chapter 3 calculates HOW MUCH memory.
% =============================================================================

\chapter{How LLMs Work}
\label{ch:how-llms-work}

\abstract*{Before diving into hardware requirements and VRAM calculations, we need to understand what's actually happening inside these models. This chapter explains transformer architecture from a systems perspective---not the math, but the components and data flow that determine memory usage and performance. By the end, you'll understand why KV cache exists, what the ``billions'' in 7B actually means, and what metrics matter for inference.}

% =============================================================================
\section{From Neural Networks to Transformers}
\label{sec:nn-to-transformers}
% =============================================================================

% TODO: Section 2.1
% - Brief neural network history (1-2 paragraphs)
% - RNNs and the sequential bottleneck
% - "Attention Is All You Need" breakthrough
% - Why transformers dominate for language (parallelization)

\subsection{The Sequential Bottleneck}
\label{subsec:sequential-bottleneck}

% TODO: RNNs process tokens one at a time, can't parallelize

\subsection{Attention Changes Everything}
\label{subsec:attention-changes}

% TODO: 2017 paper, parallel processing of all tokens

% =============================================================================
\section{Inside a Transformer}
\label{sec:inside-transformer}
% =============================================================================

% TODO: Section 2.2 - The architecture from a systems perspective

\subsection{The High-Level View}
\label{subsec:high-level-view}

% TODO: Encoder-decoder vs decoder-only, why decoder-only dominates for LLMs

\subsection{Attention: What It Actually Does}
\label{subsec:attention-mechanism}

% TODO: Explain attention without matrices
% - Each token looks at all previous tokens
% - Decides which tokens are relevant for predicting the next one
% - Query, Key, Value analogy (like a database lookup)

\subsection{Multi-Head Attention}
\label{subsec:multi-head-attention}

% TODO: Why multiple heads?
% - Different heads learn different relationships
% - One head might track syntax, another semantics
% - More heads = more parallel relationship tracking

\subsection{Feed-Forward Networks}
\label{subsec:feed-forward}

% TODO: The "thinking" layer between attention layers
% - Processes each token independently
% - Where much of the model's "knowledge" is stored

\subsection{Layers: Stacking the Blocks}
\label{subsec:layers}

% TODO:
% - Each layer = attention + feed-forward
% - Typical models: 32 layers (7B) to 80+ layers (70B+)
% - Deeper = more sophisticated reasoning

\subsection{What Are the Billions?}
\label{subsec:parameters}

% TODO: Demystify parameter counts
% - Parameters = weights = numbers the model learned during training
% - 7B = 7 billion floating point numbers
% - Where they live: attention weights, feed-forward weights, embeddings
% - Simple formula: layers × (attention params + ffn params) + embeddings

\begin{svgraybox}
\textbf{Parameter Count Breakdown (Approximate):}

For a typical 7B model with 32 layers:
\begin{itemize}
\item Embedding layer: $\sim$0.5B parameters
\item Per layer (attention + FFN): $\sim$200M parameters
\item 32 layers $\times$ 200M = $\sim$6.4B parameters
\item Output layer + other: $\sim$0.1B parameters
\item Total: $\sim$7B parameters
\end{itemize}
\end{svgraybox}

% =============================================================================
\section{The Inference Lifecycle}
\label{sec:inference-lifecycle}
% =============================================================================

% TODO: Section 2.3 - What happens from prompt to response

\subsection{Tokenization: Text to Numbers}
\label{subsec:tokenization}

% TODO:
% - Models don't see text, they see token IDs
% - Vocabulary of ~32k-128k tokens
% - Subword tokenization (BPE, SentencePiece)
% - Example: "Hello world" → [15496, 995]

\begin{lstlisting}[language=Python,caption={Tokenization example}]
# Example tokenization (conceptual)
text = "Hello, how are you?"
tokens = ["Hello", ",", " how", " are", " you", "?"]
token_ids = [15496, 11, 703, 527, 499, 30]
\end{lstlisting}

\subsection{The Prefill Phase}
\label{subsec:prefill-phase}

% TODO:
% - Processing the entire prompt at once
% - All prompt tokens processed in parallel
% - Compute-bound phase
% - Builds initial KV cache
% - This is why long prompts take time before first token

\subsection{The Decode Phase}
\label{subsec:decode-phase}

% TODO:
% - Generating one token at a time
% - Each new token depends on all previous tokens
% - Memory-bound phase (reading KV cache)
% - This is why generation speed is measured in tokens/second

\subsection{The Autoregressive Loop}
\label{subsec:autoregressive-loop}

% TODO:
% - Generate token → append to sequence → generate next token
% - Why it's called "autoregressive"
% - Each iteration: read KV cache + compute attention for new token
% - Loop until: EOS token, max length, or stop sequence

\begin{svgraybox}
\textbf{The Generation Loop:}

\begin{enumerate}
\item Process prompt (prefill) $\rightarrow$ get first token
\item Append token to sequence
\item Compute attention for new token using KV cache
\item Sample next token from probability distribution
\item Repeat steps 2-4 until stopping condition
\end{enumerate}
\end{svgraybox}

\subsection{Stopping Conditions}
\label{subsec:stopping-conditions}

% TODO:
% - End-of-sequence (EOS) token
% - Maximum token limit (num_predict/max_tokens)
% - Stop sequences (custom strings that halt generation)

% =============================================================================
\section{Why KV Cache Exists}
\label{sec:kv-cache}
% =============================================================================

% TODO: Section 2.4 - The key optimization that makes generation practical

\subsection{The Naive Approach}
\label{subsec:naive-approach}

% TODO:
% - Without optimization: recompute attention for ALL tokens every step
% - Token 100 requires computing attention over tokens 1-99
% - Token 101 requires computing attention over tokens 1-100 (redundant!)
% - Complexity: O(n²) per token generated
% - This would be impossibly slow

\subsection{The Key-Value Cache Optimization}
\label{subsec:kv-optimization}

% TODO:
% - Key insight: K and V for previous tokens don't change
% - Cache them! Only compute K,V for the new token
% - Reuse cached K,V when computing attention
% - Complexity drops dramatically

\begin{important}{The KV Cache Trade-off}
KV cache trades memory for compute. Instead of recalculating attention over all previous tokens, we store their key-value pairs and reuse them. This makes generation fast but consumes memory proportional to sequence length.
\end{important}

\subsection{How KV Cache Grows}
\label{subsec:kv-cache-growth}

% TODO:
% - Memory grows linearly with sequence length
% - Formula: 2 × layers × heads × head_dim × sequence_length × bytes_per_param
% - Example: 7B model, 4096 context, FP16 → ~1GB KV cache
% - This is why context length is limited

\begin{svgraybox}
\textbf{KV Cache Size Formula:}

\begin{equation}
\text{KV Cache (bytes)} = 2 \times L \times H \times D \times S \times B
\end{equation}

Where:
\begin{itemize}
\item $L$ = number of layers
\item $H$ = number of attention heads
\item $D$ = dimension per head
\item $S$ = sequence length
\item $B$ = bytes per value (2 for FP16)
\item Factor of 2 = one for Keys, one for Values
\end{itemize}
\end{svgraybox}

\subsection{The Memory vs Compute Trade-off}
\label{subsec:memory-compute-tradeoff}

% TODO:
% - More memory = longer contexts possible
% - During prefill: compute-bound (processing prompt)
% - During decode: memory-bound (reading KV cache)
% - This is why decode phase benefits from faster memory bandwidth

% =============================================================================
\section{Key Metrics for Inference}
\label{sec:inference-metrics}
% =============================================================================

% TODO: Section 2.5 - What to measure and why

\subsection{Time to First Token (TTFT)}
\label{subsec:ttft}

% TODO:
% - Time from request to first token of response
% - Dominated by prefill phase
% - Affected by prompt length
% - User perception: "thinking time"

\subsection{Inter-Token Latency (ITL)}
\label{subsec:itl}

% TODO:
% - Time between consecutive tokens
% - Also called Time Between Tokens (TBT)
% - Determines streaming "smoothness"
% - Target: <50ms for real-time feel

\subsection{Tokens per Second}
\label{subsec:tokens-per-second}

% TODO:
% - Generation speed: output tokens / time
% - Typical ranges: 20-100 tok/s (consumer GPU), 100-500 tok/s (datacenter)
% - Reading speed ~250 words/min ≈ 5-6 tok/s, so even slow GPUs beat reading

\subsection{Throughput}
\label{subsec:throughput}

% TODO:
% - Requests per second the system can handle
% - Different from tokens/second (system vs single request)
% - Batching increases throughput (process multiple requests)

\begin{table}[htbp]
\centering
\caption{Inference metrics summary.}
\label{tab:inference-metrics}
\begin{tabular}{llll}
\toprule
\textbf{Metric} & \textbf{Measures} & \textbf{Target} & \textbf{Affected By} \\
\midrule
TTFT & Responsiveness & <1s & Prompt length, model size \\
ITL & Streaming smoothness & <50ms & Model size, memory bandwidth \\
Tokens/sec & Generation speed & >30 & GPU power, quantization \\
Throughput & System capacity & Varies & Batching, hardware \\
\bottomrule
\end{tabular}
\end{table}

\subsection{What Affects Each Metric}
\label{subsec:metric-factors}

% TODO:
% - TTFT: prompt length (more tokens to process), model size
% - ITL: memory bandwidth (reading KV cache), model size
% - Tokens/sec: GPU compute power, quantization level
% - Throughput: batch size, continuous batching, GPU memory

% =============================================================================
\section{Other Model Architectures}
\label{sec:other-architectures}
% =============================================================================

% TODO: Section 2.6 - Brief overview of non-transformer architectures

\subsection{Mixture of Experts (MoE)}
\label{subsec:moe}

% TODO:
% - Not all parameters active for every token
% - Router selects which "experts" to use
% - Example: Mixtral 8x7B has 47B params but only 13B active
% - Implication: faster inference, but still need memory for all params

\begin{backgroundinformation}{Mixture of Experts}
MoE models like Mixtral 8x7B contain multiple ``expert'' networks. A router decides which experts handle each token. This means a 47B parameter model might only use 13B parameters per token, giving near-13B inference speed while retaining 47B model quality. The trade-off: you still need memory for all 47B parameters.
\end{backgroundinformation}

\subsection{Diffusion Models}
\label{subsec:diffusion}

% TODO:
% - Used for image/video generation (Stable Diffusion, DALL-E)
% - Different inference pattern: iterative denoising
% - Not autoregressive, no KV cache
% - Covered briefly; this book focuses on LLMs

\subsection{Multimodal Models}
\label{subsec:multimodal}

% TODO:
% - Handle multiple input types (text + images)
% - Examples: LLaVA, GPT-4V
% - Vision encoder + LLM
% - Additional memory for image embeddings

\subsection{State Space Models}
\label{subsec:ssm}

% TODO:
% - Alternative to attention (Mamba, etc.)
% - Linear complexity instead of quadratic
% - No KV cache needed
% - Emerging area, less mature tooling

% =============================================================================
\section{Summary}
\label{sec:ch02-summary}
% =============================================================================

% TODO: Write summary connecting to Chapter 3

This chapter provided the conceptual foundation for understanding LLM inference. We traced the evolution from sequential RNNs to parallel transformers, explored the attention mechanism that allows tokens to communicate, and walked through the inference lifecycle from prompt tokenization through autoregressive generation.

The key insight is the KV cache: by caching attention key-value pairs, we trade memory for dramatic compute savings. This trade-off is central to everything that follows---VRAM requirements, context length limits, and optimization strategies all flow from this fundamental architecture.

With this mental model in place, Chapter~\ref{ch:hardware-fundamentals} will quantify these concepts: exactly how much memory does a 7B model need? How does quantization reduce that? When does CPU inference make sense? The formulas will make more sense now that you understand what they're measuring.

\begin{important}{Key Takeaways}
\begin{itemize}
\item Transformers process all tokens in parallel (prefill) then generate one at a time (decode)
\item The ``billions'' in 7B are learnable parameters---weights stored as floating point numbers
\item KV cache trades memory for compute, enabling fast autoregressive generation
\item Memory grows linearly with sequence length due to KV cache
\item Key metrics: TTFT (responsiveness), ITL (smoothness), tokens/sec (speed), throughput (capacity)
\item MoE models have many parameters but only activate a subset per token
\end{itemize}
\end{important}

% =============================================================================
\section*{Problems}
\addcontentsline{toc}{section}{Problems}
% =============================================================================

\begin{prob}
\label{prob:ch02-kv-cache}
\textbf{KV Cache Calculation}\\
A model has 32 layers, 32 attention heads, 128 dimensions per head, and uses FP16 precision. Calculate the KV cache size for a 4096-token context. Then calculate for an 8192-token context.
\end{prob}

\begin{prob}
\label{prob:ch02-ttft-vs-itl}
\textbf{TTFT vs ITL}\\
Explain why a model might have excellent TTFT but poor ITL, or vice versa. What hardware characteristics would cause each scenario?
\end{prob}

\begin{prob}
\label{prob:ch02-moe-memory}
\textbf{MoE Memory Requirements}\\
Mixtral 8x7B has 8 experts with 7B parameters each, but only 2 experts are active per token. Explain why you still need memory for all 47B parameters, not just 14B.
\end{prob}

\input{chapters/references02}
