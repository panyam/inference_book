%%%%%%%%%%%%%%%%%%%%% chapter01.tex %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% Chapter 1: Introduction to Self-Hosted Inference
%
%%%%%%%%%%%%%%%%%%%%%%%% Springer-Verlag %%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Introduction to Self-Hosted Inference}
\label{ch:introduction}

\abstract*{This chapter introduces self-hosted AI inference: running models on infrastructure you control rather than consuming managed API services. We cover the fundamental differences between training and inference, the economic and strategic considerations for controlling your inference stack, and provide a roadmap for what you'll build throughout this book. By the end of this chapter, you'll run your first model and make your first inference request.}

\abstract{This chapter introduces self-hosted AI inference: running models on infrastructure you control rather than consuming managed API services. We cover the fundamental differences between training and inference, the economic and strategic considerations for controlling your inference stack, and provide a roadmap for what you'll build throughout this book. By the end of this chapter, you'll run your first model and make your first inference request.}

% =============================================================================
\section{The Rise of Self-Hosted AI}
\label{sec:rise-of-self-hosted}
% =============================================================================

The landscape of AI deployment shifted fundamentally in 2023 when Meta released Llama 2 under a permissive license~\cite{touvron2023llama2}. For the first time, a model competitive with commercial APIs was available for independent deployment---on your own terms, on infrastructure you control. Other organizations followed: Mistral AI released their 7B model in September 2023~\cite{jiang2023mistral}, Alibaba open-sourced the Qwen family~\cite{bai2023qwen}, Microsoft published the Phi series optimized for efficiency~\cite{li2023phi}, and Google contributed Gemma~\cite{team2024gemma}.

These models have modest compute requirements. A 7 billion parameter model quantized to 4-bit precision requires approximately 4GB of VRAM---achievable on a laptop GPU, a cloud instance, or a MacBook's unified memory. The barrier to entry is no longer access to models or specialized infrastructure; it's the systems knowledge required to deploy and operate them reliably.

When you use a managed API, the trade-offs are straightforward: you pay per token, accept the provider's latency (typically 50--200ms for first token), and send your data to external servers. When you control the inference stack, those trade-offs invert. Your per-request costs become predictable. Latency becomes a function of your chosen infrastructure rather than shared services and network round-trips. Your data stays within boundaries you define.

The complexity trade-off is real. Running your own inference requires understanding GPU memory hierarchies, model quantization formats, inference engine architectures, and production deployment patterns. This book provides that knowledge systematically, whether you're deploying on a cloud GPU instance, a colocated server, or a workstation under your desk.

% =============================================================================
\section{Training vs. Inference: Understanding the Difference}
\label{sec:training-vs-inference}
% =============================================================================

The terms "training" and "inference" describe two fundamentally different computational processes. Training creates a model by learning patterns from data. Inference uses that trained model to generate outputs for new inputs. The resource requirements differ by orders of magnitude, which is why you can run inference on a laptop but training typically requires a data center.

This book focuses on inference, not training. We won't derive backpropagation equations or explain attention mechanisms mathematically---other texts cover that well~\cite{goodfellow2016deep, vaswani2017attention}. Instead, we treat models as artifacts you download and deploy. The explanations below provide enough context to understand resource requirements and make informed infrastructure decisions. If terms like "parameters" or "gradients" are unfamiliar, the brief descriptions here should suffice; deeper understanding isn't required to build and operate inference systems.

\subsection{What Happens During Training}
\label{subsec:training}

Training a large language model involves adjusting billions of numerical parameters so the model can predict the next token in a sequence. The process works in three steps, repeated millions of times:

\begin{enumerate}
\item \textbf{Forward pass}: Input text flows through the network, producing a prediction for the next token.
\item \textbf{Loss calculation}: The prediction is compared to the actual next token, producing an error signal.
\item \textbf{Backward pass}: The error propagates backward through the network via gradient descent, slightly adjusting each parameter to reduce future errors.
\end{enumerate}

\begin{figure}[htbp]
\centering
\begin{tikzpicture}[
    node distance=1.5cm,
    box/.style={rectangle, draw, minimum width=2.5cm, minimum height=0.8cm, align=center},
    arrow/.style={->, >=stealth, thick}
]
    % Nodes
    \node[box] (input) {Input Tokens};
    \node[box, right=of input] (network) {Neural Network\\(7B parameters)};
    \node[box, right=of network] (pred) {Predicted\\Token};
    \node[box, below=of pred] (actual) {Actual Token};
    \node[box, below=of network] (loss) {Loss};
    \node[box, below=of input] (grad) {Gradients};

    % Forward pass (top)
    \draw[arrow, blue] (input) -- node[above, font=\small] {Forward} (network);
    \draw[arrow, blue] (network) -- (pred);

    % Loss calculation
    \draw[arrow] (pred) -- (loss);
    \draw[arrow] (actual) -- (loss);

    % Backward pass
    \draw[arrow, red] (loss) -- node[below, font=\small] {Backward} (grad);
    \draw[arrow, red, dashed] (grad) -- node[left, font=\small, align=center] {Update\\weights} (network);

    % Loop indicator
    \draw[arrow, thick, dotted] (grad.west) -- ++(-0.5,0) |- node[left, font=\small, pos=0.25] {Repeat} (input.west);
\end{tikzpicture}
\caption{The training loop. Each iteration performs a forward pass (blue) to generate predictions, computes the loss against actual targets, then propagates gradients backward (red) to update model weights. Training Llama 2 70B required approximately 1.7 million GPU hours of this loop.}
\label{fig:training-loop}
\end{figure}

The backward pass is the expensive part. It requires storing intermediate activations from the forward pass (consuming memory) and computing gradients for every parameter (consuming compute). For a 7B parameter model, this means computing and storing 7 billion gradients per training step.

Training Llama 2 70B required 1.7 million GPU hours on A100 80GB GPUs~\cite{touvron2023llama2}. At current cloud prices of approximately \$2/hour per A100, that represents over \$3 million in compute costs---before accounting for failed experiments, hyperparameter tuning, and infrastructure overhead. Training also requires massive datasets: Llama 2 used 2 trillion tokens of text data.

\subsection{What Happens During Inference}
\label{subsec:inference}

Inference is simpler: it runs only the forward pass. There's no loss calculation, no gradient computation, and no parameter updates. The model weights are fixed; you're simply using them to transform inputs into outputs.

For large language models, inference works autoregressively---the model generates one token at a time, feeding each generated token back as input to produce the next. Generating a 100-token response requires 100 forward passes through the network. This sequential dependency is why LLM inference can feel slow even on fast hardware: you cannot parallelize the generation of a single response.

The resource profile differs substantially from training:

\begin{itemize}
\item \textbf{Memory}: You need to store the model weights and a \emph{key-value cache} (KV cache) that grows with context length. No gradient storage required.
\item \textbf{Compute}: Forward passes only, with operations dominated by matrix multiplications between inputs and weights.
\item \textbf{Data}: A single prompt, not terabytes of training data.
\end{itemize}

The KV cache deserves attention because it's often the limiting factor for long-context inference. During generation, the model caches intermediate computations (keys and values from the attention mechanism) to avoid redundant work. For a 7B model with a 4096-token context, the KV cache consumes approximately 1GB of memory at FP16 precision. At 128K context length, that grows to roughly 32GB---potentially exceeding the model weights themselves. Chapter~\ref{ch:hardware} covers memory calculations in detail.

\begin{figure}[htbp]
\centering
\begin{tikzpicture}[
    node distance=0.8cm,
    box/.style={rectangle, draw, minimum width=1.8cm, minimum height=0.6cm, align=center, font=\small},
    model/.style={rectangle, draw, fill=gray!20, minimum width=1.5cm, minimum height=0.6cm, align=center, font=\small},
    cache/.style={rectangle, draw, fill=blue!10, minimum height=0.6cm, align=center, font=\scriptsize},
    arrow/.style={->, >=stealth}
]
    % Step 1
    \node[box] (p1) {Prompt};
    \node[model, right=of p1] (m1) {Model};
    \node[box, right=of m1] (t1) {Token 1};
    \draw[arrow] (p1) -- (m1);
    \draw[arrow] (m1) -- (t1);

    % Step 2
    \node[box, below=1.2cm of p1] (p2) {Prompt + T1};
    \node[model, right=of p2] (m2) {Model};
    \node[cache, below=0.1cm of m2, minimum width=0.8cm] (kv2) {KV};
    \node[box, right=of m2] (t2) {Token 2};
    \draw[arrow] (p2) -- (m2);
    \draw[arrow] (m2) -- (t2);
    \draw[arrow, dashed, gray] (t1) -- (p2);

    % Step 3
    \node[box, below=1.2cm of p2] (p3) {Prompt + T1 + T2};
    \node[model, right=of p3] (m3) {Model};
    \node[cache, below=0.1cm of m3, minimum width=1.2cm] (kv3) {KV Cache};
    \node[box, right=of m3] (t3) {Token 3};
    \draw[arrow] (p3) -- (m3);
    \draw[arrow] (m3) -- (t3);
    \draw[arrow, dashed, gray] (t2) -- (p3);

    % Labels
    \node[left=0.3cm of p1, font=\scriptsize] {Step 1:};
    \node[left=0.3cm of p2, font=\scriptsize] {Step 2:};
    \node[left=0.3cm of p3, font=\scriptsize] {Step 3:};

    % Note
    \node[below=0.5cm of kv3, font=\scriptsize, text=blue!60!black] {KV cache grows with context length};
\end{tikzpicture}
\caption{Autoregressive inference. Each token is generated by a forward pass through the model, then appended to the context for the next iteration. The KV cache stores intermediate attention computations to avoid redundant work, growing with each generated token.}
\label{fig:autoregressive-inference}
\end{figure}

A single inference request on a 7B model takes 10--100 milliseconds for the first token (depending on prompt length) and 20--50 milliseconds per subsequent token on a modern consumer GPU. This is the timescale you'll work with throughout this book: milliseconds per token, not months per training run.

\subsection{Resource Comparison}
\label{subsec:resource-comparison}

Table~\ref{tab:training-vs-inference} summarizes the resource differences. The key insight: training is a one-time cost borne by model creators (Meta, Mistral, etc.), while inference is the ongoing cost you pay when deploying models. This book assumes someone else has already trained the model; your job is to run it efficiently.

\begin{table}[htbp]
\centering
\caption{Resource requirements for training versus inference of a 70B parameter model.}
\label{tab:training-vs-inference}
\begin{tabular}{lll}
\toprule
\textbf{Aspect} & \textbf{Training} & \textbf{Inference} \\
\midrule
Hardware & 8+ A100 80GB GPUs & 1--2 consumer GPUs \\
Time per run & Weeks to months & 10--100 ms per token \\
Data required & Trillions of tokens & Single prompt \\
Memory usage & Weights + gradients + activations & Weights + KV cache \\
Compute cost & \$1--10 million & \$0.0001--0.01 per request \\
Who pays & Model creator & You (the deployer) \\
\bottomrule
\end{tabular}
\end{table}

The implication is significant: you can leverage billions of dollars of training investment by downloading open-weight models and running inference on hardware you control. A \$1,000 GPU can serve the same model that cost millions to train.

% =============================================================================
\section{Why Self-Host?}
\label{sec:why-self-host}
% =============================================================================

Organizations and individuals choose self-hosted inference for four primary reasons: cost control, data privacy, latency requirements, and customization needs. The weight of each factor varies by use case. A hobbyist experimenting on weekends has different priorities than a healthcare company processing patient records.

\subsection{Cost Control}
\label{subsec:cost-control}

API pricing follows a per-token model. As of Dec 2025, OpenAI's GPT-5 costs \$1.75 per million input tokens and \$14.00 per million output tokens~\cite{openai2025pricing}. Anthropic's Claude Sonnet 4.5 costs \$3.00 input and \$15.00 output per million tokens, while Claude Opus 4.5 runs \$5.00 input and \$25.00 output~\cite{anthropic2025pricing}. These costs accumulate quickly at scale.

Consider a customer support application handling 10,000 conversations per day, averaging 500 input tokens and 500 output
tokens per conversation. At GPT-5 rates, that's approximately \$56 per day (\$8.75 input + \$70 output), or \$2,340 per
month. For a coding assistant processing 100,000 requests daily with longer outputs, monthly API costs can reach
\$40,000--70,000 depending on the model tier.

Self-hosted costs work differently. The primary expenses are:

\begin{itemize}
\item \textbf{Hardware}: A single RTX 4090 (~ \$3,000) can run a 7B model at 30--50 tokens/second, or a quantized 70B model at 5--10 tokens/second.
\item \textbf{Electricity}: GPUs under load draw 300--400W. At \$0.12/kWh, that's roughly \$25--35/month for continuous operation.
\item \textbf{Maintenance}: Minimal for small deployments; significant for multi-GPU clusters.
\end{itemize}

The break-even calculation depends on utilization. A \$2,000 GPU investment pays for itself in 1--2 months if you're replacing \$1,500/month in API costs. If your usage is sporadic---a few hundred requests per day---APIs may remain more economical. Chapter~\ref{ch:hardware} provides detailed cost models for different deployment scales.

\subsection{Privacy and Data Sovereignty}
\label{subsec:privacy}

When you send a prompt to an external API, that data travels over the internet to servers you don't control. Most providers retain prompts and responses for some period---OpenAI's default retention is 30 days, with options to disable~\cite{openai2025dataretention}. Even with retention disabled, your data still traverses external infrastructure.

For many applications, this is acceptable. For others, it's a non-starter:

\begin{itemize}
\item \textbf{Healthcare}: Patient data falls under HIPAA in the US, requiring Business Associate Agreements (BAAs) with any third party processing protected health information. Not all API providers offer BAAs, and even those that do add compliance overhead.
\item \textbf{Legal}: Attorney-client privilege may be compromised if confidential case details are processed by external services.
\item \textbf{Finance}: Regulations like GDPR, SOX, and various banking laws impose strict requirements on where data can be processed and stored.
\item \textbf{Government}: Many agencies require data to remain within specific geographic boundaries or on classified networks.
\end{itemize}

There are two paths to addressing these concerns. First, major cloud providers offer compliant AI services---AWS, Azure, and Google Cloud all provide HIPAA-eligible and SOC 2 certified infrastructure for running inference workloads. You can deploy open models on these platforms while satisfying regulatory requirements. Second, you can run inference on your own infrastructure, whether that's a server in your office or a private cloud deployment.

This book covers both approaches. "Self-hosted" doesn't mean running your own data center---it means running inference infrastructure you control, wherever that infrastructure lives. You might deploy on a HIPAA-compliant AWS instance, a GPU server in a colocation facility, or a workstation under your desk. The common thread is that you choose the model, control the deployment, and own the infrastructure decisions.

The trade-off is responsibility. You become accountable for securing the inference infrastructure, managing access controls, and maintaining audit logs. For air-gapped environments---networks with no internet connectivity---self-hosting is the only option. For compliant cloud deployments, you inherit some security controls from the provider while retaining others.

\subsection{Latency and Performance}
\label{subsec:latency}

Managed API latency has two components: network round-trip time and inference time. A request from New York to a data center in California adds 60--80ms of network latency before inference even begins. For interactive applications---chat interfaces, code completion, real-time assistants---this overhead matters.

When you control the inference stack, you control placement. Deploy in the same region as your users, or on the same network as your application servers, and network latency drops to single-digit milliseconds. For latency-critical workloads, you can colocate inference with the consuming application.

Consistency is equally important. Managed APIs serve many customers on shared infrastructure. During peak demand, you compete for resources with other users, leading to variable response times. Your P99 latency might be 3x your median latency. With dedicated infrastructure---whether a reserved cloud instance or on-premise hardware---your latency distribution tightens because you're not sharing resources.

The performance trade-off depends on model size and hardware. A 7B model on a mid-range GPU delivers 30--50 tokens per second with time-to-first-token under 100ms. That's competitive with managed APIs for most workloads. Larger models on slower hardware may underperform managed services that run on cutting-edge GPUs with aggressive optimization. Chapter~\ref{ch:hardware} helps you match hardware to performance requirements.

\subsection{Customization and Control}
\label{subsec:customization}

Managed APIs expose a fixed set of models with predetermined parameters. You get what the provider offers. When you control the stack, you choose everything:

\begin{itemize}
\item \textbf{Model selection}: Run any open-weight model---Llama, Mistral, Qwen, Phi, or specialized models for code, math, or specific languages. Switch models without changing providers or renegotiating contracts.
\item \textbf{Fine-tuning}: Adapt models to your domain with your data. A legal firm can fine-tune on case law; a medical organization can specialize for clinical terminology. The fine-tuned weights stay on your infrastructure.
\item \textbf{Inference parameters}: Control temperature, top-p, repetition penalties, and stopping conditions precisely. Some parameters available on your own deployment aren't exposed by managed APIs.
\item \textbf{No quotas}: Managed APIs impose rate limits and usage caps, especially on newer or more capable models. Your infrastructure has no artificial limits---only the throughput your hardware can sustain.
\item \textbf{Version pinning}: Lock to a specific model version for reproducibility. Managed APIs occasionally update models, subtly changing behavior. With your own deployment, the model only changes when you change it.
\end{itemize}

This control matters most when AI is core to your product rather than a peripheral feature. If model behavior directly affects your business outcomes, controlling that behavior becomes a strategic priority.

\subsection{When NOT to Self-Host}
\label{subsec:when-not-to-self-host}

Self-hosting isn't always the right choice. Managed APIs win in several scenarios:

\begin{itemize}
\item \textbf{Low or unpredictable volume}: If you're making hundreds of requests per day rather than thousands, the operational overhead of maintaining inference infrastructure exceeds the cost savings. APIs let you pay only for what you use.
\item \textbf{Experimentation phase}: When you're still figuring out whether AI adds value to your product, managed APIs let you iterate without infrastructure commitment. Build the prototype first; optimize deployment later.
\item \textbf{Cutting-edge models}: The best open-weight models lag behind frontier closed models by 6--12 months. If you need GPT-5 or Claude Opus capabilities today, APIs are your only option.
\item \textbf{Multimodal requirements}: Open models for vision, audio, and video generation are maturing but still trail proprietary alternatives in quality. Complex multimodal pipelines may require API access.
\item \textbf{Limited engineering capacity}: Running inference infrastructure requires systems engineering skills---debugging GPU memory issues, optimizing throughput, maintaining uptime. If your team lacks this expertise and can't acquire it, the learning curve may not justify the benefits.
\end{itemize}

The decision isn't binary. Many organizations use managed APIs for experimentation and frontier capabilities while self-hosting for high-volume production workloads with stable requirements. Chapter~\ref{ch:hardware} includes a decision framework for evaluating which workloads to self-host.

% =============================================================================
\section{What You'll Build in This Book}
\label{sec:what-youll-build}
% =============================================================================

This book teaches inference systems by building one. You'll construct a control plane---the software layer that manages model serving---and evolve it from a simple proxy to a production-grade platform. The models grow alongside the infrastructure: from 7B parameters on a single GPU to 400B distributed across a cluster.

\subsection{The Progressive Journey}
\label{subsec:progressive-journey}

The book is organized into four parts, each targeting a larger model size and adding capabilities to the control plane:

\begin{description}
\item[Part I: Foundations (7B)] You'll run your first model, understand hardware requirements, learn model formats and quantization, compare inference engines, and build Control Plane v0.1---a basic proxy with health checks and metrics. A single consumer GPU or cloud instance handles everything.

\item[Part II: Production (30B)] The control plane gains auth, rate limiting, caching, and request queuing. You'll deploy 30B models requiring careful memory management and learn optimization techniques that squeeze maximum throughput from your hardware.

\item[Part III: Multi-Tenant (70B)] Control Plane v0.3 supports multiple tenants with isolated quotas, usage tracking, and billing integration. You'll distribute 70B models across multiple GPUs using tensor parallelism and build routing logic that directs requests to appropriate backends.

\item[Part IV: Inference Lab (400B)] The capstone: a 400B parameter deployment on H100 GPUs, serving a coding assistant called CodeLab. Control Plane v1.0 includes everything needed to operate inference at scale---the complete system you've built across the book.
\end{description}

\subsection{Control Plane Evolution}
\label{subsec:control-plane-evolution}

The control plane is written in Go. This choice is deliberate: Go compiles to a single binary with no runtime dependencies, making deployment straightforward. Its concurrency model handles many simultaneous inference requests efficiently. And Go is readable---you don't need deep language expertise to understand or modify the code.

The architecture uses interfaces to abstract components. An \texttt{InferenceBackend} interface defines how the control plane talks to inference engines; implementations exist for Ollama, llama.cpp, and vLLM. When you add vLLM support in Part II, you implement the interface without changing the rest of the system. This pattern repeats throughout: \texttt{RateLimiter}, \texttt{Cache}, \texttt{Router}, \texttt{BillingProvider}---each is an interface with swappable implementations.

\begin{table}[htbp]
\centering
\caption{Control plane evolution across the book.}
\label{tab:control-plane-versions}
\begin{tabular}{llp{7cm}}
\toprule
\textbf{Version} & \textbf{Part} & \textbf{Key Capabilities} \\
\midrule
v0.1 & I & Single backend, health checks, Prometheus metrics \\
v0.2 & II & Auth, rate limiting, caching, request queue \\
v0.3 & III & Multi-tenant, usage tracking, billing, model routing \\
v1.0 & IV & Distributed inference, advanced scheduling, complete platform \\
\bottomrule
\end{tabular}
\end{table}

Each version builds on the previous. Code from Chapter 5 remains in the final system; it just gains neighbors. This mirrors how production systems actually evolve---incrementally, with backward compatibility.

\subsection{What You'll Learn Beyond the Code}
\label{subsec:beyond-the-code}

The control plane is the tangible artifact, but the deeper goal is building intuition. You'll learn enough about transformer memory layout to calculate VRAM requirements for any model. You'll understand quantization well enough to choose between Q4\_K\_M and Q5\_K\_S for your workload. You'll grasp attention mechanisms sufficiently to reason about KV cache growth and context length trade-offs.

This isn't a machine learning theory book---we won't derive backpropagation or prove convergence bounds. But we cover the systems-relevant math: memory calculations, throughput modeling, parallelism strategies. When new techniques emerge---mixture of experts, speculative decoding, expert parallelism---you'll have the mental framework to evaluate them. Chapter~\ref{ch:30b-optimization} implements expert parallelism from scratch, not just to use it, but to understand when and why it helps.

The field moves fast. Models that seem large today will be routine tomorrow. The specific numbers in this book will date; the systems thinking won't. Our aim is engineers who can read a new model's architecture paper and estimate what hardware it needs, or evaluate a new inference engine and predict where it will excel.

% =============================================================================
\section{Hands-On: Your First Inference}
\label{sec:hands-on-first-inference}
% =============================================================================

Enough theory---let's run a model. We'll use Ollama, an inference server that handles model downloads, format conversion, and serving behind a simple API. It's not the fastest option (we'll cover vLLM and llama.cpp later), but it's the easiest path from zero to working inference.

By the end of this section, you'll have a 7B parameter model running and responding to requests.

\subsection{Installing Ollama}
\label{subsec:installing-ollama}

Ollama runs on macOS, Linux, and Windows. Installation takes under a minute:

\begin{description}
\item[macOS] Install via Homebrew:
\begin{lstlisting}[language=bash]
brew install ollama
\end{lstlisting}

\item[Linux] Use the install script:
\begin{lstlisting}[language=bash]
curl -fsSL https://ollama.com/install.sh | sh
\end{lstlisting}

\item[Windows] Download the installer from \url{https://ollama.com/download/windows}.
\end{description}

After installation, start the Ollama server:

\begin{lstlisting}[language=bash]
ollama serve
\end{lstlisting}

The server runs on \texttt{localhost:11434} by default. In a separate terminal, verify it's running:

\begin{lstlisting}[language=bash]
curl http://localhost:11434/api/tags
\end{lstlisting}

You should see an empty model list: \texttt{\{"models":[]\}}. We'll fix that next.

\subsection{Running Your First Model}
\label{subsec:first-model}

Ollama downloads models on demand from its registry. We'll use Qwen 2.5, Alibaba's open model family that offers strong performance across reasoning, code, and multilingual tasks~\cite{qwen2025}. The 7B variant balances capability with modest hardware requirements:

\begin{lstlisting}[language=bash]
ollama pull qwen2.5:7b
\end{lstlisting}

The download is approximately 4.7GB (the model is quantized to 4-bit precision by default). Once complete, you can interact with it directly:

\begin{lstlisting}[language=bash]
ollama run qwen2.5:7b
\end{lstlisting}

This opens an interactive chat session. Type a message, press Enter, and watch tokens stream back. Behind the scenes, Ollama loads the model weights into memory (GPU if available, CPU otherwise), processes your input through the transformer, and generates tokens autoregressively---exactly the process we described in Section~\ref{sec:training-vs-inference}.

Type \texttt{/bye} to exit the session.

To see what models you have locally:

\begin{lstlisting}[language=bash]
ollama list
\end{lstlisting}

The output shows model name, size, and when it was last modified. As you work through this book, you'll accumulate models here---different sizes, different quantizations, different families. We use Qwen throughout the book for consistency; Appendix~\ref{app:mistral} provides equivalent commands for Mistral models if you prefer a European alternative.

\subsection{Making API Requests}
\label{subsec:api-requests}

The interactive CLI is useful for experimentation, but applications call Ollama's HTTP API. The server exposes REST endpoints for generation, chat, and model management.

A basic generation request:

\begin{lstlisting}[language=bash]
curl http://localhost:11434/api/generate \
  -d '{
    "model": "qwen2.5:7b",
    "prompt": "Explain what AI inference is in one paragraph.",
    "stream": false
  }'
\end{lstlisting}

The response is JSON containing the generated text and timing metadata:

\begin{lstlisting}
{
  "model": "qwen2.5:7b",
  "response": "AI inference refers to the process of ....",
  "done": true,
  "context": [ 151644, 8948, 198, ... ],
  "total_duration": 1930969541,
  "load_duration": 41119208,
  "prompt_eval_count": 39,
  "prompt_eval_duration": 112548500,
  "eval_count": 80,
  "eval_duration": 1776717000
}
\end{lstlisting}

Setting \texttt{stream: false} waits for the complete response. For interactive applications, streaming provides better perceived latency---users see tokens as they're generated:

\begin{lstlisting}[language=bash]
curl http://localhost:11434/api/generate \
  -d '{"model": "qwen2.5:7b", "prompt": "Write a haiku.", "stream": true}'
\end{lstlisting}

This returns newline-delimited JSON objects, one per generated token.

For chat-style interactions with conversation history, use the \texttt{/api/chat} endpoint:

\begin{lstlisting}[language=bash]
curl http://localhost:11434/api/chat \
  -d '{
    "model": "qwen2.5:7b",
    "messages": [
      {"role": "user", "content": "What is the capital of France?"}
    ],
    "stream": false
  }'
\end{lstlisting}

The API accepts additional parameters---\texttt{temperature}, \texttt{top\_p}, \texttt{num\_predict}---covered in Chapter~\ref{ch:inference-engines}. The defaults work well for initial experimentation.

% =============================================================================
\section{Understanding the Response}
\label{sec:understanding-response}
% =============================================================================

The response from your first inference contains more than just text. The metadata reveals how the model processed your request---information that becomes essential for optimization and debugging.

\subsection{Tokens and Tokenization}
\label{subsec:tokens}

Language models don't process text character by character. They operate on \emph{tokens}---subword units that balance vocabulary size against sequence length. The word "understanding" might be a single token; "tokenization" might split into "token" + "ization." Common words compress to single tokens; rare words decompose into pieces.

Different models use different tokenizers. Qwen 2.5 uses a byte-pair encoding (BPE) tokenizer with approximately 150,000 tokens in its vocabulary. This affects everything from input costs (APIs charge per token) to context limits (models have token budgets, not character budgets).

In the response above, \texttt{prompt\_eval\_count: 39} indicates your prompt consumed 39 tokens. The phrase "Explain
what AI inference is in one paragraph." contains 8 words but 39 tokens---the tokenizer made different choices than word
boundaries. The field \texttt{eval\_count: 80} shows the model generated 80 tokens in response.

You can inspect tokenization directly:

\begin{lstlisting}[language=bash]
curl http://localhost:11434/api/generate \
  -d '{"model": "qwen2.5:7b", "prompt": "Hello world", "raw": true}'
\end{lstlisting}

Understanding tokenization matters for three reasons. First, cost: API pricing is per-token, so knowing how your prompts tokenize helps budget. Second, context: a 4096-token context window accommodates different amounts of text depending on content. Code typically tokenizes more efficiently than prose; non-English text often tokenizes less efficiently. Third, performance: longer token sequences mean more computation. A 1000-token prompt requires more processing than a 100-token prompt, affecting latency.

\subsection{Timing Metrics}
\label{subsec:timing-metrics}

The response includes timing data in nanoseconds:

\begin{itemize}
\item \texttt{total\_duration}: Wall-clock time for the entire request---1.93 seconds in our example.
\item \texttt{eval\_duration}: Time spent generating tokens---1.78 seconds. The difference (150ms) covers prompt processing and overhead.
\end{itemize}

From these numbers, you can calculate \emph{tokens per second}: $80 \text{ tokens} / 1.78 \text{ seconds} \approx 45 \text{ tok/s}$. This metric---generation throughput---is the primary performance indicator for inference systems. Higher is better; it directly affects how fast users see responses.

Two latency metrics matter for interactive applications:

\begin{description}
\item[Time to First Token (TTFT)] How long until the first generated token appears. For streaming responses, this is perceived latency---users see something immediately even if generation continues. TTFT includes prompt processing time.
\item[Inter-Token Latency (ITL)] Average time between generated tokens. With 80 tokens in 1.78 seconds, ITL is approximately 15ms. Lower ITL means smoother streaming.
\end{description}

Hardware dramatically affects these numbers. The same Qwen 2.5 7B model might achieve 20 tok/s on CPU, 75 tok/s on an RTX 4060, and 150 tok/s on an RTX 4090. Chapter~\ref{ch:hardware} provides benchmarks across GPU tiers.

\subsection{Generation Parameters}
\label{subsec:generation-params}

Beyond the prompt, several parameters control how the model generates text. These aren't visible in our simple request because Ollama used defaults, but they're available when you need them:

\begin{itemize}
\item \textbf{temperature}: Controls randomness. At 0, the model always picks the highest-probability token (deterministic). At 1.0, sampling follows the probability distribution. Higher values (up to 2.0) increase creativity and unpredictability. Default is typically 0.7--0.8.
\item \textbf{top\_p} (nucleus sampling): Restricts sampling to tokens whose cumulative probability exceeds this threshold. At 0.9, the model considers only the most likely tokens that together account for 90\% of probability mass. Lower values increase focus; higher values allow more variety.
\item \textbf{num\_predict}: Maximum tokens to generate. Limits response length. The model may stop earlier if it generates a stop token.
\end{itemize}

For most applications, defaults work well. Adjust temperature for creative tasks (higher) or factual extraction (lower). Chapter~\ref{ch:inference-engines} explores these parameters in depth with guidance on tuning for specific use cases.

% =============================================================================
\section{Summary}
\label{sec:ch01-summary}
% =============================================================================

This chapter established the foundation for everything that follows. You now understand the fundamental distinction between training and inference: training creates models through expensive, iterative optimization across massive datasets; inference uses those models for prediction through forward passes alone. This asymmetry---training costs millions, inference runs on a laptop---is why self-hosted inference is practical.

You've evaluated the trade-offs. Self-hosting offers predictable costs, data control, lower latency, and freedom to customize. It requires systems knowledge and operational responsibility. Managed APIs remain appropriate for low-volume workloads, cutting-edge models, and teams without infrastructure expertise. The choice depends on your requirements---and often, organizations use both.

Most importantly, you've run your first model. Qwen 2.5 7B is now responding to requests on your machine. You understand the response structure: tokens as the atomic unit, timing metrics that reveal performance, and parameters that control generation behavior.

\begin{important}{Key Takeaways}
\begin{itemize}
\item Inference runs only forward passes---no gradients, no training data, achievable on consumer hardware.
\item Self-hosted inference means controlling your stack, whether that's cloud instances, colocated servers, or local workstations.
\item Open models (Qwen, Mistral, Llama, Phi, Gemma) make self-hosting viable for many production workloads.
\item Tokens are the fundamental unit: they affect cost, context limits, and performance.
\item Key metrics: tokens per second (throughput), time to first token (responsiveness), inter-token latency (streaming smoothness).
\end{itemize}
\end{important}

\subsection*{Next: Hardware}

Chapter~\ref{ch:hardware} answers the question you're likely asking: what hardware do I actually need? We'll calculate memory requirements for different model sizes, compare GPU tiers and their price-performance ratios, and help you choose infrastructure that matches your workload. You'll learn why a 7B model fits in 4GB but a 70B model needs 40GB---and what quantization has to do with it.

% =============================================================================
\section*{Problems}
\addcontentsline{toc}{section}{Problems}
% =============================================================================

\begin{prob}
\label{prob:ch01-cost-analysis}
\textbf{Cost Comparison}\\
Calculate the monthly cost of running 100,000 inference requests through OpenAI's GPT-4 API versus self-hosting a 7B model on an RTX 4060. Assume average request length of 500 tokens input and 200 tokens output.
\end{prob}

\begin{prob}
\label{prob:ch01-latency-measurement}
\textbf{Latency Measurement}\\
Using Ollama, measure the time-to-first-token (TTFT) and tokens-per-second for three different prompts: (a) a simple question, (b) a code generation request, and (c) a creative writing prompt. What patterns do you observe?
\end{prob}

\begin{prob}
\label{prob:ch01-streaming}
\textbf{Streaming vs Non-Streaming}\\
Implement a simple Python script that makes both streaming and non-streaming requests to Ollama. Measure the perceived latency (time until user sees first output) for each approach.
\end{prob}

\input{chapters/references01}
