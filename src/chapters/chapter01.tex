%%%%%%%%%%%%%%%%%%%%% chapter01.tex %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% Chapter 1: Introduction to Self-Hosted Inference
%
%%%%%%%%%%%%%%%%%%%%%%%% Springer-Verlag %%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Introduction to Self-Hosted Inference}
\label{ch:introduction}

\abstract*{This chapter introduces self-hosted AI inference: running models on infrastructure you control rather than consuming managed API services. We cover the fundamental differences between training and inference, the economic and strategic considerations for controlling your inference stack, and provide a roadmap for what you'll build throughout this book. By the end of this chapter, you'll run your first model and make your first inference request.}

\abstract{This chapter introduces self-hosted AI inference: running models on infrastructure you control rather than consuming managed API services. We cover the fundamental differences between training and inference, the economic and strategic considerations for controlling your inference stack, and provide a roadmap for what you'll build throughout this book. By the end of this chapter, you'll run your first model and make your first inference request.}

% =============================================================================
\section{The Rise of Self-Hosted AI}
\label{sec:rise-of-self-hosted}
% =============================================================================

The landscape of AI deployment shifted fundamentally in 2023 when Meta released Llama 2 under a permissive license~\cite{touvron2023llama2}. For the first time, a model competitive with commercial APIs was available for independent deployment---on your own terms, on infrastructure you control. Other organizations followed: Mistral AI released their 7B model in September 2023~\cite{jiang2023mistral}, Alibaba open-sourced the Qwen family~\cite{bai2023qwen}, Microsoft published the Phi series optimized for efficiency~\cite{li2023phi}, and Google contributed Gemma~\cite{team2024gemma}.

These models have modest compute requirements. A 7 billion parameter model quantized to 4-bit precision requires approximately 4GB of VRAM---achievable on a laptop GPU, a cloud instance, or a MacBook's unified memory. The barrier to entry is no longer access to models or specialized infrastructure; it's the systems knowledge required to deploy and operate them reliably.

When you use a managed API, the trade-offs are straightforward: you pay per token, accept the provider's latency (typically 50--200ms for first token), and send your data to external servers. When you control the inference stack, those trade-offs invert. Your per-request costs become predictable. Latency becomes a function of your chosen infrastructure rather than shared services and network round-trips. Your data stays within boundaries you define.

The complexity trade-off is real. Running your own inference requires understanding GPU memory hierarchies, model quantization formats, inference engine architectures, and production deployment patterns. This book provides that knowledge systematically, whether you're deploying on a cloud GPU instance, a colocated server, or a workstation under your desk.

% =============================================================================
\section{Training vs. Inference: Understanding the Difference}
\label{sec:training-vs-inference}
% =============================================================================

The terms "training" and "inference" describe two fundamentally different computational processes. Training creates a model by learning patterns from data. Inference uses that trained model to generate outputs for new inputs. The resource requirements differ by orders of magnitude, which is why you can run inference on a laptop but training typically requires a data center.

This book focuses on inference, not training. We won't derive backpropagation equations or explain attention mechanisms mathematically---other texts cover that well~\cite{goodfellow2016deep, vaswani2017attention}. Instead, we treat models as artifacts you download and deploy. The explanations below provide enough context to understand resource requirements and make informed infrastructure decisions. If terms like "parameters" or "gradients" are unfamiliar, the brief descriptions here should suffice; deeper understanding isn't required to build and operate inference systems.

\subsection{What Happens During Training}
\label{subsec:training}

Training a large language model involves adjusting billions of numerical parameters so the model can predict the next token in a sequence. The process works in three steps, repeated millions of times:

\begin{enumerate}
\item \textbf{Forward pass}: Input text flows through the network, producing a prediction for the next token.
\item \textbf{Loss calculation}: The prediction is compared to the actual next token, producing an error signal.
\item \textbf{Backward pass}: The error propagates backward through the network via gradient descent, slightly adjusting each parameter to reduce future errors.
\end{enumerate}

\begin{figure}[htbp]
\centering
\begin{tikzpicture}[
    node distance=1.5cm,
    box/.style={rectangle, draw, minimum width=2.5cm, minimum height=0.8cm, align=center},
    arrow/.style={->, >=stealth, thick}
]
    % Nodes
    \node[box] (input) {Input Tokens};
    \node[box, right=of input] (network) {Neural Network\\(7B parameters)};
    \node[box, right=of network] (pred) {Predicted\\Token};
    \node[box, below=of pred] (actual) {Actual Token};
    \node[box, below=of network] (loss) {Loss};
    \node[box, below=of input] (grad) {Gradients};

    % Forward pass (top)
    \draw[arrow, blue] (input) -- node[above, font=\small] {Forward} (network);
    \draw[arrow, blue] (network) -- (pred);

    % Loss calculation
    \draw[arrow] (pred) -- (loss);
    \draw[arrow] (actual) -- (loss);

    % Backward pass
    \draw[arrow, red] (loss) -- node[below, font=\small] {Backward} (grad);
    \draw[arrow, red, dashed] (grad) -- node[left, font=\small, align=center] {Update\\weights} (network);

    % Loop indicator
    \draw[arrow, thick, dotted] (grad.west) -- ++(-0.5,0) |- node[left, font=\small, pos=0.25] {Repeat} (input.west);
\end{tikzpicture}
\caption{The training loop. Each iteration performs a forward pass (blue) to generate predictions, computes the loss against actual targets, then propagates gradients backward (red) to update model weights. Training Llama 2 70B required approximately 1.7 million GPU hours of this loop.}
\label{fig:training-loop}
\end{figure}

The backward pass is the expensive part. It requires storing intermediate activations from the forward pass (consuming memory) and computing gradients for every parameter (consuming compute). For a 7B parameter model, this means computing and storing 7 billion gradients per training step.

Training Llama 2 70B required 1.7 million GPU hours on A100 80GB GPUs~\cite{touvron2023llama2}. At current cloud prices of approximately \$2/hour per A100, that represents over \$3 million in compute costs---before accounting for failed experiments, hyperparameter tuning, and infrastructure overhead. Training also requires massive datasets: Llama 2 used 2 trillion tokens of text data.

\subsection{What Happens During Inference}
\label{subsec:inference}

Inference is simpler: it runs only the forward pass. There's no loss calculation, no gradient computation, and no parameter updates. The model weights are fixed; you're simply using them to transform inputs into outputs.

For large language models, inference works autoregressively---the model generates one token at a time, feeding each generated token back as input to produce the next. Generating a 100-token response requires 100 forward passes through the network. This sequential dependency is why LLM inference can feel slow even on fast hardware: you cannot parallelize the generation of a single response.

The resource profile differs substantially from training:

\begin{itemize}
\item \textbf{Memory}: You need to store the model weights and a \emph{key-value cache} (KV cache) that grows with context length. No gradient storage required.
\item \textbf{Compute}: Forward passes only, with operations dominated by matrix multiplications between inputs and weights.
\item \textbf{Data}: A single prompt, not terabytes of training data.
\end{itemize}

The KV cache deserves attention because it's often the limiting factor for long-context inference. During generation, the model caches intermediate computations (keys and values from the attention mechanism) to avoid redundant work. For a 7B model with a 4096-token context, the KV cache consumes approximately 1GB of memory at FP16 precision. At 128K context length, that grows to roughly 32GB---potentially exceeding the model weights themselves. Chapter~\ref{ch:hardware} covers memory calculations in detail.

\begin{figure}[htbp]
\centering
\begin{tikzpicture}[
    node distance=0.8cm,
    box/.style={rectangle, draw, minimum width=1.8cm, minimum height=0.6cm, align=center, font=\small},
    model/.style={rectangle, draw, fill=gray!20, minimum width=1.5cm, minimum height=0.6cm, align=center, font=\small},
    cache/.style={rectangle, draw, fill=blue!10, minimum height=0.6cm, align=center, font=\scriptsize},
    arrow/.style={->, >=stealth}
]
    % Step 1
    \node[box] (p1) {Prompt};
    \node[model, right=of p1] (m1) {Model};
    \node[box, right=of m1] (t1) {Token 1};
    \draw[arrow] (p1) -- (m1);
    \draw[arrow] (m1) -- (t1);

    % Step 2
    \node[box, below=1.2cm of p1] (p2) {Prompt + T1};
    \node[model, right=of p2] (m2) {Model};
    \node[cache, below=0.1cm of m2, minimum width=0.8cm] (kv2) {KV};
    \node[box, right=of m2] (t2) {Token 2};
    \draw[arrow] (p2) -- (m2);
    \draw[arrow] (m2) -- (t2);
    \draw[arrow, dashed, gray] (t1) -- (p2);

    % Step 3
    \node[box, below=1.2cm of p2] (p3) {Prompt + T1 + T2};
    \node[model, right=of p3] (m3) {Model};
    \node[cache, below=0.1cm of m3, minimum width=1.2cm] (kv3) {KV Cache};
    \node[box, right=of m3] (t3) {Token 3};
    \draw[arrow] (p3) -- (m3);
    \draw[arrow] (m3) -- (t3);
    \draw[arrow, dashed, gray] (t2) -- (p3);

    % Labels
    \node[left=0.3cm of p1, font=\scriptsize] {Step 1:};
    \node[left=0.3cm of p2, font=\scriptsize] {Step 2:};
    \node[left=0.3cm of p3, font=\scriptsize] {Step 3:};

    % Note
    \node[below=0.5cm of kv3, font=\scriptsize, text=blue!60!black] {KV cache grows with context length};
\end{tikzpicture}
\caption{Autoregressive inference. Each token is generated by a forward pass through the model, then appended to the context for the next iteration. The KV cache stores intermediate attention computations to avoid redundant work, growing with each generated token.}
\label{fig:autoregressive-inference}
\end{figure}

A single inference request on a 7B model takes 10--100 milliseconds for the first token (depending on prompt length) and 20--50 milliseconds per subsequent token on a modern consumer GPU. This is the timescale you'll work with throughout this book: milliseconds per token, not months per training run.

\subsection{Resource Comparison}
\label{subsec:resource-comparison}

Table~\ref{tab:training-vs-inference} summarizes the resource differences. The key insight: training is a one-time cost borne by model creators (Meta, Mistral, etc.), while inference is the ongoing cost you pay when deploying models. This book assumes someone else has already trained the model; your job is to run it efficiently.

\begin{table}[htbp]
\centering
\caption{Resource requirements for training versus inference of a 70B parameter model.}
\label{tab:training-vs-inference}
\begin{tabular}{lll}
\toprule
\textbf{Aspect} & \textbf{Training} & \textbf{Inference} \\
\midrule
Hardware & 8+ A100 80GB GPUs & 1--2 consumer GPUs \\
Time per run & Weeks to months & 10--100 ms per token \\
Data required & Trillions of tokens & Single prompt \\
Memory usage & Weights + gradients + activations & Weights + KV cache \\
Compute cost & \$1--10 million & \$0.0001--0.01 per request \\
Who pays & Model creator & You (the deployer) \\
\bottomrule
\end{tabular}
\end{table}

The implication is significant: you can leverage billions of dollars of training investment by downloading open-weight models and running inference on hardware you control. A \$1,000 GPU can serve the same model that cost millions to train.

% =============================================================================
\section{Why Self-Host?}
\label{sec:why-self-host}
% =============================================================================

Organizations and individuals choose self-hosted inference for four primary reasons: cost control, data privacy, latency requirements, and customization needs. The weight of each factor varies by use case. A hobbyist experimenting on weekends has different priorities than a healthcare company processing patient records.

\subsection{Cost Control}
\label{subsec:cost-control}

API pricing follows a per-token model. As of Dec 2025, OpenAI's GPT-5 costs \$1.75 per million input tokens and \$14.00 per million output tokens~\cite{openai2025pricing}. Anthropic's Claude Sonnet 4.5 costs \$3.00 input and \$15.00 output per million tokens, while Claude Opus 4.5 runs \$5.00 input and \$25.00 output~\cite{anthropic2025pricing}. These costs accumulate quickly at scale.

Consider a customer support application handling 10,000 conversations per day, averaging 500 input tokens and 500 output
tokens per conversation. At GPT-5 rates, that's approximately \$56 per day (\$8.75 input + \$70 output), or \$2,340 per
month. For a coding assistant processing 100,000 requests daily with longer outputs, monthly API costs can reach
\$40,000--70,000 depending on the model tier.

Self-hosted costs work differently. The primary expenses are:

\begin{itemize}
\item \textbf{Hardware}: A single RTX 4090 (~ \$3,000) can run a 7B model at 30--50 tokens/second, or a quantized 70B model at 5--10 tokens/second.
\item \textbf{Electricity}: GPUs under load draw 300--400W. At \$0.12/kWh, that's roughly \$25--35/month for continuous operation.
\item \textbf{Maintenance}: Minimal for small deployments; significant for multi-GPU clusters.
\end{itemize}

The break-even calculation depends on utilization. A \$2,000 GPU investment pays for itself in 1--2 months if you're replacing \$1,500/month in API costs. If your usage is sporadic---a few hundred requests per day---APIs may remain more economical. Chapter~\ref{ch:hardware} provides detailed cost models for different deployment scales.

\subsection{Privacy and Data Sovereignty}
\label{subsec:privacy}

When you send a prompt to an external API, that data travels over the internet to servers you don't control. Most providers retain prompts and responses for some period---OpenAI's default retention is 30 days, with options to disable~\cite{openai2025dataretention}. Even with retention disabled, your data still traverses external infrastructure.

For many applications, this is acceptable. For others, it's a non-starter:

\begin{itemize}
\item \textbf{Healthcare}: Patient data falls under HIPAA in the US, requiring Business Associate Agreements (BAAs) with any third party processing protected health information. Not all API providers offer BAAs, and even those that do add compliance overhead.
\item \textbf{Legal}: Attorney-client privilege may be compromised if confidential case details are processed by external services.
\item \textbf{Finance}: Regulations like GDPR, SOX, and various banking laws impose strict requirements on where data can be processed and stored.
\item \textbf{Government}: Many agencies require data to remain within specific geographic boundaries or on classified networks.
\end{itemize}

There are two paths to addressing these concerns. First, major cloud providers offer compliant AI services---AWS, Azure, and Google Cloud all provide HIPAA-eligible and SOC 2 certified infrastructure for running inference workloads. You can deploy open models on these platforms while satisfying regulatory requirements. Second, you can run inference on your own infrastructure, whether that's a server in your office or a private cloud deployment.

This book covers both approaches. "Self-hosted" doesn't mean running your own data center---it means running inference infrastructure you control, wherever that infrastructure lives. You might deploy on a HIPAA-compliant AWS instance, a GPU server in a colocation facility, or a workstation under your desk. The common thread is that you choose the model, control the deployment, and own the infrastructure decisions.

The trade-off is responsibility. You become accountable for securing the inference infrastructure, managing access controls, and maintaining audit logs. For air-gapped environments---networks with no internet connectivity---self-hosting is the only option. For compliant cloud deployments, you inherit some security controls from the provider while retaining others.

\subsection{Latency and Performance}
\label{subsec:latency}

Managed API latency has two components: network round-trip time and inference time. A request from New York to a data center in California adds 60--80ms of network latency before inference even begins. For interactive applications---chat interfaces, code completion, real-time assistants---this overhead matters.

When you control the inference stack, you control placement. Deploy in the same region as your users, or on the same network as your application servers, and network latency drops to single-digit milliseconds. For latency-critical workloads, you can colocate inference with the consuming application.

Consistency is equally important. Managed APIs serve many customers on shared infrastructure. During peak demand, you compete for resources with other users, leading to variable response times. Your P99 latency might be 3x your median latency. With dedicated infrastructure---whether a reserved cloud instance or on-premise hardware---your latency distribution tightens because you're not sharing resources.

The performance trade-off depends on model size and hardware. A 7B model on a mid-range GPU delivers 30--50 tokens per second with time-to-first-token under 100ms. That's competitive with managed APIs for most workloads. Larger models on slower hardware may underperform managed services that run on cutting-edge GPUs with aggressive optimization. Chapter~\ref{ch:hardware} helps you match hardware to performance requirements.

\subsection{Customization and Control}
\label{subsec:customization}

Managed APIs expose a fixed set of models with predetermined parameters. You get what the provider offers. When you control the stack, you choose everything:

\begin{itemize}
\item \textbf{Model selection}: Run any open-weight model---Llama, Mistral, Qwen, Phi, or specialized models for code, math, or specific languages. Switch models without changing providers or renegotiating contracts.
\item \textbf{Fine-tuning}: Adapt models to your domain with your data. A legal firm can fine-tune on case law; a medical organization can specialize for clinical terminology. The fine-tuned weights stay on your infrastructure.
\item \textbf{Inference parameters}: Control temperature, top-p, repetition penalties, and stopping conditions precisely. Some parameters available on your own deployment aren't exposed by managed APIs.
\item \textbf{No quotas}: Managed APIs impose rate limits and usage caps, especially on newer or more capable models. Your infrastructure has no artificial limits---only the throughput your hardware can sustain.
\item \textbf{Version pinning}: Lock to a specific model version for reproducibility. Managed APIs occasionally update models, subtly changing behavior. With your own deployment, the model only changes when you change it.
\end{itemize}

This control matters most when AI is core to your product rather than a peripheral feature. If model behavior directly affects your business outcomes, controlling that behavior becomes a strategic priority.

\subsection{When NOT to Self-Host}
\label{subsec:when-not-to-self-host}

Self-hosting isn't always the right choice. Managed APIs win in several scenarios:

\begin{itemize}
\item \textbf{Low or unpredictable volume}: If you're making hundreds of requests per day rather than thousands, the operational overhead of maintaining inference infrastructure exceeds the cost savings. APIs let you pay only for what you use.
\item \textbf{Experimentation phase}: When you're still figuring out whether AI adds value to your product, managed APIs let you iterate without infrastructure commitment. Build the prototype first; optimize deployment later.
\item \textbf{Cutting-edge models}: The best open-weight models lag behind frontier closed models by 6--12 months. If you need GPT-5 or Claude Opus capabilities today, APIs are your only option.
\item \textbf{Multimodal requirements}: Open models for vision, audio, and video generation are maturing but still trail proprietary alternatives in quality. Complex multimodal pipelines may require API access.
\item \textbf{Limited engineering capacity}: Running inference infrastructure requires systems engineering skills---debugging GPU memory issues, optimizing throughput, maintaining uptime. If your team lacks this expertise and can't acquire it, the learning curve may not justify the benefits.
\end{itemize}

The decision isn't binary. Many organizations use managed APIs for experimentation and frontier capabilities while self-hosting for high-volume production workloads with stable requirements. Chapter~\ref{ch:hardware} includes a decision framework for evaluating which workloads to self-host.

% =============================================================================
\section{What You'll Build in This Book}
\label{sec:what-youll-build}
% =============================================================================

This book teaches inference systems by building one. You'll construct a control plane---the software layer that manages model serving---and evolve it from a simple proxy to a production-grade platform. The models grow alongside the infrastructure: from 7B parameters on a single GPU to 400B distributed across a cluster.

\subsection{The Progressive Journey}
\label{subsec:progressive-journey}

The book is organized into four parts, each targeting a larger model size and adding capabilities to the control plane:

\begin{description}
\item[Part I: Foundations (7B)] You'll run your first model, understand hardware requirements, learn model formats and quantization, compare inference engines, and build Control Plane v0.1---a basic proxy with health checks and metrics. A single consumer GPU or cloud instance handles everything.

\item[Part II: Production (30B)] The control plane gains authentication, rate limiting, caching, and request queuing. You'll deploy 30B models that require more careful memory management and learn optimization techniques that squeeze maximum throughput from your hardware.

\item[Part III: Multi-Tenant (70B)] Control Plane v0.3 supports multiple tenants with isolated quotas, usage tracking, and billing integration. You'll distribute 70B models across multiple GPUs using tensor parallelism and build routing logic that directs requests to appropriate backends.

\item[Part IV: Inference Lab (400B)] The capstone: a 400B parameter deployment on H100 GPUs, serving a coding assistant called CodeLab. Control Plane v1.0 includes everything needed to operate inference at scale---the complete system you've built across the book.
\end{description}

\subsection{Control Plane Evolution}
\label{subsec:control-plane-evolution}

The control plane is written in Go. This choice is deliberate: Go compiles to a single binary with no runtime dependencies, making deployment straightforward. Its concurrency model handles many simultaneous inference requests efficiently. And Go is readable---you don't need deep language expertise to understand or modify the code.

The architecture uses interfaces to abstract components. An \texttt{InferenceBackend} interface defines how the control plane talks to inference engines; implementations exist for Ollama, llama.cpp, and vLLM. When you add vLLM support in Part II, you implement the interface without changing the rest of the system. This pattern repeats throughout: \texttt{RateLimiter}, \texttt{Cache}, \texttt{Router}, \texttt{BillingProvider}---each is an interface with swappable implementations.

\begin{table}[htbp]
\centering
\caption{Control plane evolution across the book.}
\label{tab:control-plane-versions}
\begin{tabular}{llp{7cm}}
\toprule
\textbf{Version} & \textbf{Part} & \textbf{Key Capabilities} \\
\midrule
v0.1 & I & Single backend, health checks, Prometheus metrics \\
v0.2 & II & Auth, rate limiting, caching, request queue \\
v0.3 & III & Multi-tenant, usage tracking, billing, model routing \\
v1.0 & IV & Distributed inference, advanced scheduling, complete platform \\
\bottomrule
\end{tabular}
\end{table}

Each version builds on the previous. Code from Chapter 5 remains in the final system; it just gains neighbors. This mirrors how production systems actually evolve---incrementally, with backward compatibility.

\subsection{What You'll Learn Beyond the Code}
\label{subsec:beyond-the-code}

The control plane is the tangible artifact, but the deeper goal is building intuition. You'll learn enough about transformer memory layout to calculate VRAM requirements for any model. You'll understand quantization well enough to choose between Q4\_K\_M and Q5\_K\_S for your workload. You'll grasp attention mechanisms sufficiently to reason about KV cache growth and context length trade-offs.

This isn't a machine learning theory book---we won't derive backpropagation or prove convergence bounds. But we cover the systems-relevant math: memory calculations, throughput modeling, parallelism strategies. When new techniques emerge---mixture of experts, speculative decoding, expert parallelism---you'll have the mental framework to evaluate them. Chapter~\ref{ch:optimization} implements expert parallelism from scratch, not just to use it, but to understand when and why it helps.

The field moves fast. Models that seem large today will be routine tomorrow. The specific numbers in this book will date; the systems thinking won't. Our aim is engineers who can read a new model's architecture paper and estimate what hardware it needs, or evaluate a new inference engine and predict where it will excel.

% =============================================================================
\section{Hands-On: Your First Local Inference}
\label{sec:hands-on-first-inference}
% =============================================================================

% TODO: Step-by-step guide

\subsection{Installing Ollama}
\label{subsec:installing-ollama}

% TODO: Installation instructions
% - macOS: brew install ollama
% - Linux: curl script
% - Windows: installer
% - Verify installation

\subsection{Running Your First Model}
\label{subsec:first-model}

% TODO: Running a model
% - ollama pull llama3.2:7b
% - ollama run llama3.2:7b
% - Understanding what's happening

\subsection{Making API Requests}
\label{subsec:api-requests}

% TODO: API usage
% - curl examples
% - Python example
% - Response structure
% - Streaming vs non-streaming

\begin{programcode}{First Inference Request}
\begin{lstlisting}[language=bash]
# Pull a 7B model
ollama pull llama3.2:7b

# Make an inference request
curl http://localhost:11434/api/generate \
  -d '{
    "model": "llama3.2:7b",
    "prompt": "Explain what AI inference is in one paragraph.",
    "stream": false
  }'
\end{lstlisting}
\end{programcode}

% =============================================================================
\section{Understanding the Response}
\label{sec:understanding-response}
% =============================================================================

% TODO: Explain response structure
% - Tokens and tokenization
% - Generation parameters
% - Metrics (tokens/second, timing)

% =============================================================================
\section{Summary}
\label{sec:ch01-summary}
% =============================================================================

% TODO: Chapter summary
% - Key concepts covered
% - What you've accomplished
% - Preview of next chapter

\begin{important}{Key Takeaways}
\begin{itemize}
\item Inference is fundamentally different from training and accessible with consumer hardware
\item Self-hosting offers cost control, privacy, and customization benefits
\item Open models like Llama, Mistral, and Qwen make self-hosting viable
\item You've run your first local inference with Ollama
\end{itemize}
\end{important}

% =============================================================================
\section*{Problems}
\addcontentsline{toc}{section}{Problems}
% =============================================================================

\begin{prob}
\label{prob:ch01-cost-analysis}
\textbf{Cost Comparison}\\
Calculate the monthly cost of running 100,000 inference requests through OpenAI's GPT-4 API versus self-hosting a 7B model on an RTX 4060. Assume average request length of 500 tokens input and 200 tokens output.
\end{prob}

\begin{prob}
\label{prob:ch01-latency-measurement}
\textbf{Latency Measurement}\\
Using Ollama, measure the time-to-first-token (TTFT) and tokens-per-second for three different prompts: (a) a simple question, (b) a code generation request, and (c) a creative writing prompt. What patterns do you observe?
\end{prob}

\begin{prob}
\label{prob:ch01-streaming}
\textbf{Streaming vs Non-Streaming}\\
Implement a simple Python script that makes both streaming and non-streaming requests to Ollama. Measure the perceived latency (time until user sees first output) for each approach.
\end{prob}

\input{chapters/references01}
