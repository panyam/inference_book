%%%%%%%%%%%%%%%%%%%%% chapter05.tex %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% Chapter 5: Building Control Plane v0.1
%
%%%%%%%%%%%%%%%%%%%%%%%% Springer-Verlag %%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Building Control Plane v0.1}
\label{ch:control-plane-v01}

\abstract*{This chapter is where theory meets practice. We build the first version of our Go-based control plane---a foundation that will evolve throughout the book. Control Plane v0.1 provides a single backend proxy, unified API, health checks, metrics collection, and Docker deployment with Prometheus and Grafana. By the end, you'll have a working inference system with observability built in from day one.}

\abstract{This chapter is where theory meets practice. We build the first version of our Go-based control plane---a foundation that will evolve throughout the book. Control Plane v0.1 provides a single backend proxy, unified API, health checks, metrics collection, and Docker deployment with Prometheus and Grafana. By the end, you'll have a working inference system with observability built in from day one.}

% =============================================================================
\section{Control Plane Architecture}
\label{sec:cp-architecture}
% =============================================================================

% TODO: High-level architecture overview

\subsection{What is a Control Plane?}
\label{subsec:what-is-control-plane}

% TODO: Define the concept
% - Separating control from data flow
% - Why abstraction matters
% - Evolution path

\subsection{v0.1 Capabilities}
\label{subsec:v01-capabilities}

% TODO: What we're building
% - Single backend proxy
% - Unified API (OpenAI-compatible)
% - Health checks
% - Metrics (Prometheus)
% - Configuration management

\subsection{System Architecture Diagram}
\label{subsec:architecture-diagram}

% TODO: ASCII or figure showing:
% Client -> Control Plane -> Inference Engine (Ollama/vLLM)
%                |
%                v
%           Prometheus -> Grafana

% =============================================================================
\section{Project Setup}
\label{sec:project-setup}
% =============================================================================

% TODO: Setting up the Go project

\subsection{Go Module Initialization}
\label{subsec:go-module}

\begin{programcode}{Project Structure}
\begin{lstlisting}[language=bash]
# Initialize project
mkdir inference-control-plane
cd inference-control-plane
go mod init github.com/yourorg/inference-control-plane

# Create directory structure
mkdir -p cmd/server
mkdir -p internal/{api,backend,config,health,metrics}
mkdir -p deployments/docker
mkdir -p configs
\end{lstlisting}
\end{programcode}

\subsection{Dependencies}
\label{subsec:dependencies}

% TODO: Key dependencies
% - chi or standard library router
% - prometheus client
% - structured logging (slog)
% - configuration (viper or similar)

% =============================================================================
\section{Core Interfaces}
\label{sec:core-interfaces}
% =============================================================================

% TODO: Define the foundational interfaces

\subsection{Backend Interface}
\label{subsec:backend-interface}

\begin{programcode}{Backend Interface}
\begin{lstlisting}[language=Go]
// internal/backend/backend.go

package backend

import "context"

// GenerateRequest represents an inference request
type GenerateRequest struct {
    Model       string   `json:"model"`
    Prompt      string   `json:"prompt"`
    MaxTokens   int      `json:"max_tokens,omitempty"`
    Temperature float64  `json:"temperature,omitempty"`
    Stream      bool     `json:"stream,omitempty"`
    Stop        []string `json:"stop,omitempty"`
}

// GenerateResponse represents an inference response
type GenerateResponse struct {
    ID      string `json:"id"`
    Object  string `json:"object"`
    Created int64  `json:"created"`
    Model   string `json:"model"`
    Choices []Choice `json:"choices"`
    Usage   Usage  `json:"usage"`
}

type Choice struct {
    Index        int    `json:"index"`
    Text         string `json:"text,omitempty"`
    Message      *Message `json:"message,omitempty"`
    FinishReason string `json:"finish_reason"`
}

type Message struct {
    Role    string `json:"role"`
    Content string `json:"content"`
}

type Usage struct {
    PromptTokens     int `json:"prompt_tokens"`
    CompletionTokens int `json:"completion_tokens"`
    TotalTokens      int `json:"total_tokens"`
}

// Backend defines the interface for inference backends
type Backend interface {
    // Generate performs text generation
    Generate(ctx context.Context, req GenerateRequest) (GenerateResponse, error)

    // Health checks if the backend is healthy
    Health(ctx context.Context) error

    // Name returns the backend identifier
    Name() string
}
\end{lstlisting}
\end{programcode}

\subsection{Health Interface}
\label{subsec:health-interface}

\begin{programcode}{Health Check Interface}
\begin{lstlisting}[language=Go]
// internal/health/health.go

package health

import "context"

// Status represents health status
type Status string

const (
    StatusHealthy   Status = "healthy"
    StatusDegraded  Status = "degraded"
    StatusUnhealthy Status = "unhealthy"
)

// CheckResult contains a health check result
type CheckResult struct {
    Name    string `json:"name"`
    Status  Status `json:"status"`
    Message string `json:"message,omitempty"`
    Latency int64  `json:"latency_ms"`
}

// Checker defines a health check component
type Checker interface {
    Check(ctx context.Context) CheckResult
    Name() string
}

// Aggregator combines multiple health checks
type Aggregator interface {
    Register(checker Checker)
    CheckAll(ctx context.Context) []CheckResult
    OverallStatus(ctx context.Context) Status
}
\end{lstlisting}
\end{programcode}

% =============================================================================
\section{Implementing the Ollama Backend}
\label{sec:ollama-backend}
% =============================================================================

% TODO: Concrete implementation

\begin{programcode}{Ollama Backend Implementation}
\begin{lstlisting}[language=Go]
// internal/backend/ollama.go

package backend

import (
    "bytes"
    "context"
    "encoding/json"
    "fmt"
    "net/http"
    "time"
)

type OllamaBackend struct {
    baseURL    string
    httpClient *http.Client
    model      string
}

func NewOllamaBackend(baseURL, model string) *OllamaBackend {
    return &OllamaBackend{
        baseURL: baseURL,
        model:   model,
        httpClient: &http.Client{
            Timeout: 5 * time.Minute,
        },
    }
}

func (b *OllamaBackend) Name() string {
    return "ollama"
}

func (b *OllamaBackend) Generate(ctx context.Context, req GenerateRequest) (GenerateResponse, error) {
    // Convert to Ollama format
    ollamaReq := map[string]interface{}{
        "model":  req.Model,
        "prompt": req.Prompt,
        "stream": false,
    }

    if req.MaxTokens > 0 {
        ollamaReq["num_predict"] = req.MaxTokens
    }
    if req.Temperature > 0 {
        ollamaReq["temperature"] = req.Temperature
    }

    body, err := json.Marshal(ollamaReq)
    if err != nil {
        return GenerateResponse{}, fmt.Errorf("marshal request: %w", err)
    }

    httpReq, err := http.NewRequestWithContext(
        ctx,
        http.MethodPost,
        b.baseURL+"/api/generate",
        bytes.NewReader(body),
    )
    if err != nil {
        return GenerateResponse{}, fmt.Errorf("create request: %w", err)
    }
    httpReq.Header.Set("Content-Type", "application/json")

    resp, err := b.httpClient.Do(httpReq)
    if err != nil {
        return GenerateResponse{}, fmt.Errorf("execute request: %w", err)
    }
    defer resp.Body.Close()

    if resp.StatusCode != http.StatusOK {
        return GenerateResponse{}, fmt.Errorf("ollama returned status %d", resp.StatusCode)
    }

    var ollamaResp struct {
        Response string `json:"response"`
    }
    if err := json.NewDecoder(resp.Body).Decode(&ollamaResp); err != nil {
        return GenerateResponse{}, fmt.Errorf("decode response: %w", err)
    }

    // Convert to standard format
    return GenerateResponse{
        ID:      fmt.Sprintf("gen-%d", time.Now().UnixNano()),
        Object:  "text_completion",
        Created: time.Now().Unix(),
        Model:   req.Model,
        Choices: []Choice{{
            Index:        0,
            Text:         ollamaResp.Response,
            FinishReason: "stop",
        }},
    }, nil
}

func (b *OllamaBackend) Health(ctx context.Context) error {
    req, err := http.NewRequestWithContext(ctx, http.MethodGet, b.baseURL+"/api/tags", nil)
    if err != nil {
        return err
    }

    resp, err := b.httpClient.Do(req)
    if err != nil {
        return err
    }
    defer resp.Body.Close()

    if resp.StatusCode != http.StatusOK {
        return fmt.Errorf("unhealthy: status %d", resp.StatusCode)
    }
    return nil
}
\end{lstlisting}
\end{programcode}

% =============================================================================
\section{API Server}
\label{sec:api-server}
% =============================================================================

% TODO: HTTP API implementation

\subsection{Router Setup}
\label{subsec:router-setup}

% TODO: Setting up routes
% - POST /v1/completions
% - POST /v1/chat/completions
% - GET /health
% - GET /metrics

\subsection{Request Handling}
\label{subsec:request-handling}

% TODO: Middleware and handlers

% =============================================================================
\section{Metrics with Prometheus}
\label{sec:prometheus-metrics}
% =============================================================================

% TODO: Metrics implementation

\subsection{Key Metrics to Track}
\label{subsec:key-metrics}

% TODO: What to measure
% - Request count
% - Request latency
% - Token throughput
% - Error rates
% - Backend health

\begin{programcode}{Metrics Definition}
\begin{lstlisting}[language=Go]
// internal/metrics/metrics.go

package metrics

import (
    "github.com/prometheus/client_golang/prometheus"
    "github.com/prometheus/client_golang/prometheus/promauto"
)

var (
    RequestsTotal = promauto.NewCounterVec(
        prometheus.CounterOpts{
            Name: "inference_requests_total",
            Help: "Total number of inference requests",
        },
        []string{"model", "status"},
    )

    RequestDuration = promauto.NewHistogramVec(
        prometheus.HistogramOpts{
            Name:    "inference_request_duration_seconds",
            Help:    "Request duration in seconds",
            Buckets: []float64{0.1, 0.5, 1, 2, 5, 10, 30, 60},
        },
        []string{"model"},
    )

    TokensGenerated = promauto.NewCounterVec(
        prometheus.CounterOpts{
            Name: "inference_tokens_generated_total",
            Help: "Total tokens generated",
        },
        []string{"model"},
    )

    BackendHealth = promauto.NewGaugeVec(
        prometheus.GaugeOpts{
            Name: "inference_backend_health",
            Help: "Backend health status (1 = healthy, 0 = unhealthy)",
        },
        []string{"backend"},
    )
)
\end{lstlisting}
\end{programcode}

% =============================================================================
\section{Docker Deployment}
\label{sec:docker-deployment}
% =============================================================================

% TODO: Docker setup

\subsection{Dockerfile}
\label{subsec:dockerfile}

% TODO: Multi-stage build

\subsection{Docker Compose Stack}
\label{subsec:docker-compose}

\begin{programcode}{Docker Compose}
\begin{lstlisting}[language=bash]
# deployments/docker/docker-compose.yml

version: '3.8'

services:
  control-plane:
    build:
      context: ../..
      dockerfile: deployments/docker/Dockerfile
    ports:
      - "8080:8080"
    environment:
      - BACKEND_URL=http://ollama:11434
      - METRICS_PORT=9090
    depends_on:
      - ollama
      - prometheus

  ollama:
    image: ollama/ollama:latest
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

  prometheus:
    image: prom/prometheus:latest
    ports:
      - "9091:9090"
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus_data:/prometheus

  grafana:
    image: grafana/grafana:latest
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
    volumes:
      - grafana_data:/var/lib/grafana
      - ./grafana/provisioning:/etc/grafana/provisioning

volumes:
  ollama_data:
  prometheus_data:
  grafana_data:
\end{lstlisting}
\end{programcode}

% =============================================================================
\section{Grafana Dashboard}
\label{sec:grafana-dashboard}
% =============================================================================

% TODO: Dashboard creation
% - Request rate panel
% - Latency percentiles
% - Error rate
% - Token throughput
% - Backend health

% =============================================================================
\section{Testing the Control Plane}
\label{sec:testing-control-plane}
% =============================================================================

% TODO: Integration testing
% - Starting the stack
% - Making test requests
% - Verifying metrics
% - Checking dashboards

% =============================================================================
\section{Configuration Management}
\label{sec:configuration}
% =============================================================================

% TODO: Config file and environment variables

\begin{programcode}{Configuration}
\begin{lstlisting}[language=Go]
// internal/config/config.go

package config

type Config struct {
    Server  ServerConfig  `yaml:"server"`
    Backend BackendConfig `yaml:"backend"`
    Metrics MetricsConfig `yaml:"metrics"`
}

type ServerConfig struct {
    Host string `yaml:"host" env:"SERVER_HOST" default:"0.0.0.0"`
    Port int    `yaml:"port" env:"SERVER_PORT" default:"8080"`
}

type BackendConfig struct {
    Type    string `yaml:"type" env:"BACKEND_TYPE" default:"ollama"`
    URL     string `yaml:"url" env:"BACKEND_URL" default:"http://localhost:11434"`
    Model   string `yaml:"model" env:"BACKEND_MODEL" default:"llama3.2:7b"`
    Timeout int    `yaml:"timeout" env:"BACKEND_TIMEOUT" default:"300"`
}

type MetricsConfig struct {
    Enabled bool   `yaml:"enabled" env:"METRICS_ENABLED" default:"true"`
    Path    string `yaml:"path" env:"METRICS_PATH" default:"/metrics"`
}
\end{lstlisting}
\end{programcode}

% =============================================================================
\section{Summary}
\label{sec:ch05-summary}
% =============================================================================

\begin{important}{Key Takeaways}
\begin{itemize}
\item Control Plane v0.1 provides abstraction over inference backends
\item Interface-driven design enables future extensibility
\item Observability (metrics + health checks) is built-in from day one
\item Docker Compose provides a complete development environment
\item This foundation will evolve throughout the book
\end{itemize}
\end{important}

\begin{svgraybox}
\textbf{What's Next:}

In the optional Chapter 5.5, we'll add browser-based inference for zero-cost deployments. In Part II, we'll enhance this control plane with authentication, rate limiting, caching, and multi-model routing for 30B+ production deployments.
\end{svgraybox}

% =============================================================================
\section*{Problems}
\addcontentsline{toc}{section}{Problems}
% =============================================================================

\begin{prob}
\label{prob:ch05-vllm-backend}
\textbf{vLLM Backend Implementation}\\
Implement a \texttt{VLLMBackend} struct that satisfies the \texttt{Backend} interface. It should communicate with vLLM's OpenAI-compatible API.
\end{prob}

\begin{prob}
\label{prob:ch05-streaming}
\textbf{Streaming Support}\\
Extend the \texttt{Backend} interface to support streaming responses. Implement streaming for the Ollama backend.
\end{prob}

\begin{prob}
\label{prob:ch05-dashboard}
\textbf{Custom Grafana Dashboard}\\
Create a Grafana dashboard that displays: (a) request rate over time, (b) p50/p95/p99 latency, (c) tokens per second, and (d) error rate. Export the dashboard as JSON.
\end{prob}

\input{chapters/chapter06/references}
