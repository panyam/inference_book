%%%%%%%%%%%%%%%%%%%%% chapter03.tex %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% Chapter 3: Model Formats and Quantization
%
%%%%%%%%%%%%%%%%%%%%%%%% Springer-Verlag %%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Model Formats and Quantization}
\label{ch:models}

\abstract*{The choice of model format and quantization level directly impacts performance, quality, and resource requirements. This chapter explains the major model formats (SafeTensors, GGUF, ONNX), demystifies quantization techniques (GPTQ, AWQ, GGML/GGUF), provides guidance on choosing models for different use cases, and introduces the concept of a model registry for your control plane. We focus on the practical knowledge needed to make informed model selection decisions.}

\abstract{The choice of model format and quantization level directly impacts performance, quality, and resource requirements. This chapter explains the major model formats (SafeTensors, GGUF, ONNX), demystifies quantization techniques (GPTQ, AWQ, GGML/GGUF), provides guidance on choosing models for different use cases, and introduces the concept of a model registry for your control plane. We focus on the practical knowledge needed to make informed model selection decisions.}

% =============================================================================
\section{Understanding Model Formats}
\label{sec:model-formats}
% =============================================================================

% TODO: Overview of formats and their purposes

\subsection{SafeTensors}
\label{subsec:safetensors}

% TODO: The modern standard
% - What it is
% - Safety advantages over pickle
% - Where it's used (HuggingFace standard)
% - Loading and inspection

\subsection{GGUF (GPT-Generated Unified Format)}
\label{subsec:gguf}

% TODO: The llama.cpp format
% - History: GGML -> GGUF
% - Single-file advantage
% - Metadata embedding
% - Quantization built-in
% - Where to find GGUF models

\subsection{ONNX (Open Neural Network Exchange)}
\label{subsec:onnx}

% TODO: Cross-platform format
% - Purpose and advantages
% - When to use
% - Conversion considerations

\subsection{Format Comparison}
\label{subsec:format-comparison}

% TODO: Comparison table
% | Format | Use Case | Quantization | Ecosystem |
% | SafeTensors | Training, HF | External | PyTorch, TF |
% | GGUF | Inference | Built-in | llama.cpp |
% | ONNX | Cross-platform | External | Multiple |

% =============================================================================
\section{Quantization Fundamentals}
\label{sec:quantization-fundamentals}
% =============================================================================

% TODO: What quantization is and why it matters

\subsection{What is Quantization?}
\label{subsec:what-is-quantization}

% TODO: Core concepts
% - Reducing precision
% - Memory savings
% - Speed improvements
% - Quality trade-offs

\subsection{Quantization Levels}
\label{subsec:quantization-levels}

% TODO: Common quantization levels
% - Q8 (8-bit)
% - Q6 (6-bit)
% - Q5 (5-bit)
% - Q4 (4-bit) - sweet spot
% - Q3/Q2 (aggressive)

\begin{svgraybox}
\textbf{The Quantization Sweet Spot:}

For most use cases, Q4 (4-bit) quantization provides the best balance:
\begin{itemize}
\item 4x memory reduction vs FP16
\item Minimal quality degradation (typically <3\%)
\item Sufficient precision for most tasks
\item Good inference speed
\end{itemize}

Go lower (Q3/Q2) only when memory-constrained. Go higher (Q5/Q6) for quality-critical applications.
\end{svgraybox}

% =============================================================================
\section{Quantization Methods}
\label{sec:quantization-methods}
% =============================================================================

% TODO: Different quantization approaches

\subsection{GGUF Quantization (llama.cpp)}
\label{subsec:gguf-quantization}

% TODO: GGUF quantization types
% - Q4_K_M, Q4_K_S explained
% - Q5_K_M, Q5_K_S
% - What the K, M, S mean
% - Choosing the right variant

\subsection{GPTQ}
\label{subsec:gptq}

% TODO: GPTQ quantization
% - How it works (calibration-based)
% - Advantages: accuracy preservation
% - Disadvantages: requires GPU inference
% - When to use

\subsection{AWQ (Activation-aware Weight Quantization)}
\label{subsec:awq}

% TODO: AWQ quantization
% - How it differs from GPTQ
% - Performance characteristics
% - Ecosystem support

\subsection{BitsAndBytes}
\label{subsec:bitsandbytes}

% TODO: HuggingFace integration
% - 8-bit and 4-bit options
% - nf4 quantization
% - When to use

\subsection{Method Comparison}
\label{subsec:quant-method-comparison}

% TODO: When to use each
% | Method | Ecosystem | CPU Support | GPU Support | Quality |
% | GGUF | llama.cpp | Yes | Yes | Good |
% | GPTQ | HF/vLLM | No | Yes | Excellent |
% | AWQ | vLLM | No | Yes | Excellent |
% | BnB | HF | Limited | Yes | Good |

% =============================================================================
\section{Choosing Models for Your Use Case}
\label{sec:choosing-models}
% =============================================================================

% TODO: Decision framework

\subsection{Model Families Overview}
\label{subsec:model-families}

% TODO: Current landscape
% - Llama family (Meta)
% - Mistral family
% - Qwen family (Alibaba)
% - Phi family (Microsoft)
% - Gemma family (Google)
% - Open-source vs open-weight

\subsection{Size vs Quality Trade-offs}
\label{subsec:size-quality}

% TODO: How to think about model size
% - Benchmarks and their limitations
% - Real-world testing importance
% - Task-specific performance

\subsection{Decision Matrix}
\label{subsec:model-decision-matrix}

% TODO: Practical recommendations
% | Task | Min Size | Recommended | Notes |
% | Simple chat | 1B | 7B | TinyLlama, Phi-3 |
% | Code generation | 7B | 13B-33B | CodeLlama, Qwen-Coder |
% | Complex reasoning | 13B | 70B | Llama 3.1, Mistral |
% | Production | 7B-13B | 30B | Balance quality/cost |

% =============================================================================
\section{Where to Find Models}
\label{sec:finding-models}
% =============================================================================

% TODO: Model sources

\subsection{Hugging Face Hub}
\label{subsec:huggingface-hub}

% TODO: The main repository
% - Navigation
% - Finding quantized versions
% - Evaluating model cards
% - TheBloke's quantizations

\subsection{Ollama Library}
\label{subsec:ollama-library}

% TODO: Ollama's curated collection
% - Pre-configured models
% - Version management
% - Custom modelfiles

\subsection{Model Evaluation}
\label{subsec:model-evaluation}

% TODO: How to evaluate models
% - Benchmark scores (limitations)
% - Your own test suite
% - A/B testing

% =============================================================================
\section{Building a Model Registry}
\label{sec:model-registry}
% =============================================================================

% TODO: Introduce the concept for control plane

\subsection{Why a Registry?}
\label{subsec:why-registry}

% TODO: Benefits
% - Centralized metadata
% - Version tracking
% - Capability mapping
% - Cost tracking

\subsection{Registry Data Model}
\label{subsec:registry-data-model}

\begin{programcode}{Model Registry Interface (Go)}
\begin{lstlisting}[language=Go]
// ModelInfo represents metadata about a model
type ModelInfo struct {
    ID            string            `json:"id"`
    Name          string            `json:"name"`
    Family        string            `json:"family"`
    Parameters    string            `json:"parameters"` // e.g., "7B"
    Quantization  string            `json:"quantization"` // e.g., "Q4_K_M"
    Format        string            `json:"format"` // gguf, safetensors
    VRAMRequired  int64             `json:"vram_required_mb"`
    ContextWindow int               `json:"context_window"`
    Capabilities  []string          `json:"capabilities"`
    CostPerToken  float64           `json:"cost_per_token"`
    Metadata      map[string]string `json:"metadata"`
}

// ModelRegistry manages available models
type ModelRegistry interface {
    List(ctx context.Context) ([]ModelInfo, error)
    Get(ctx context.Context, modelID string) (ModelInfo, error)
    Register(ctx context.Context, model ModelInfo) error
    FindByCapability(ctx context.Context, cap string) ([]ModelInfo, error)
}
\end{lstlisting}
\end{programcode}

% =============================================================================
\section{Summary}
\label{sec:ch03-summary}
% =============================================================================

\begin{important}{Key Takeaways}
\begin{itemize}
\item GGUF is the format of choice for flexible inference (CPU + GPU support)
\item Q4 quantization provides the best quality/memory trade-off for most cases
\item GPTQ/AWQ offer superior quality but require GPU-only inference
\item Model choice depends on task complexity, resource constraints, and quality needs
\item A model registry helps manage the growing ecosystem of options
\end{itemize}
\end{important}

% =============================================================================
\section*{Problems}
\addcontentsline{toc}{section}{Problems}
% =============================================================================

\begin{prob}
\label{prob:ch03-quantization-comparison}
\textbf{Quantization Quality Comparison}\\
Download the same base model in Q4, Q5, and Q8 quantization. Using a consistent set of 20 prompts (10 factual, 10 creative), evaluate the output quality. Document your methodology and findings.
\end{prob}

\begin{prob}
\label{prob:ch03-model-selection}
\textbf{Model Selection for Use Case}\\
You're building a customer support chatbot that needs to handle billing questions, technical troubleshooting, and appointment scheduling. You have 16GB VRAM available. Research and recommend a specific model, justifying your choice.
\end{prob}

\begin{prob}
\label{prob:ch03-registry-implementation}
\textbf{Model Registry Implementation}\\
Implement the \texttt{ModelRegistry} interface using a JSON file as the backing store. Include methods for adding models, listing by capability, and finding the best model for a given VRAM constraint.
\end{prob}

\input{chapters/references03}
