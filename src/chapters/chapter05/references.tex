%%%%%%%%%%%%%%%%%%%%%%%% references05.tex %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% References for Chapter 5: Inference Engines
%%%%%%%%%%%%%%%%%%%%%%%% Springer-Verlag %%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{thebibliography}{99.}

\bibitem{kwon2023vllm}
Kwon, W., Li, Z., Zhuang, S., Sheng, Y., Zheng, L., Yu, C.H., Gonzalez, J.E., Zhang, H., and Stoica, I. (2023).
Efficient Memory Management for Large Language Model Serving with PagedAttention.
In \emph{Proceedings of the 29th Symposium on Operating Systems Principles (SOSP '23)}.
ACM.
\url{https://arxiv.org/abs/2309.06180}

\bibitem{dao2022flashattention}
Dao, T., Fu, D.Y., Ermon, S., Rudra, A., and RÃ©, C. (2022).
FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness.
In \emph{Advances in Neural Information Processing Systems 35 (NeurIPS 2022)}.
\url{https://arxiv.org/abs/2205.14135}

\bibitem{zheng2024sglang}
Zheng, L., Yin, L., Xie, Z., Huang, J., Sun, C., Yu, C.H., Cao, S., Kozyrakis, C., Stoica, I., Gonzalez, J.E., Barrett, C., and Sheng, Y. (2024).
SGLang: Efficient Execution of Structured Language Model Programs.
\url{https://arxiv.org/abs/2312.07104}

\bibitem{leviathan2023speculative}
Leviathan, Y., Kalman, M., and Matias, Y. (2023).
Fast Inference from Transformers via Speculative Decoding.
In \emph{Proceedings of the 40th International Conference on Machine Learning (ICML 2023)}.
\url{https://arxiv.org/abs/2211.17192}

\bibitem{nvidia2024trtllm}
NVIDIA. (2024).
TensorRT-LLM: A TensorRT Toolbox for Optimized Large Language Model Inference.
NVIDIA Developer Documentation.
\url{https://nvidia.github.io/TensorRT-LLM/}

\bibitem{gerganov2023llamacpp}
Gerganov, G. (2023).
llama.cpp: Port of Facebook's LLaMA model in C/C++.
GitHub Repository.
\url{https://github.com/ggerganov/llama.cpp}

\bibitem{ollama2024}
Ollama. (2024).
Ollama: Get up and running with large language models locally.
\url{https://ollama.com}

\bibitem{yu2022orca}
Yu, G.I., Jeong, J.S., Kim, G.W., Kim, S., and Chun, B.G. (2022).
Orca: A Distributed Serving System for Transformer-Based Generative Models.
In \emph{Proceedings of the 16th USENIX Symposium on Operating Systems Design and Implementation (OSDI '22)}.
\url{https://www.usenix.org/conference/osdi22/presentation/yu}

\bibitem{agrawal2024taming}
Agrawal, A., Kedia, N., Panwar, A., Mohan, J., Kwatra, N., Gulavani, B., Ramjee, R., and Tumanov, A. (2024).
Taming Throughput-Latency Tradeoff in LLM Inference with Sarathi-Serve.
\url{https://arxiv.org/abs/2403.02310}

\end{thebibliography}
