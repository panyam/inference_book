%%%%%%%%%%%%%%%%%%%%% chapter04.tex %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% Chapter 4: Inference Engines
%
%%%%%%%%%%%%%%%%%%%%%%%% Springer-Verlag %%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Inference Engines}
\label{ch:inference-engines}

\abstract*{An inference engine is the core component that actually runs model computations. This chapter provides a deep dive into the major inference engines: Ollama for simplicity, llama.cpp for flexibility, and vLLM for production performance. We explain when to use each, how to configure them, and how they integrate with your control plane through their APIs.}

\abstract{An inference engine is the core component that actually runs model computations. This chapter provides a deep dive into the major inference engines: Ollama for simplicity, llama.cpp for flexibility, and vLLM for production performance. We explain when to use each, how to configure them, and how they integrate with your control plane through their APIs.}

% =============================================================================
\section{The Role of Inference Engines}
\label{sec:inference-engine-role}
% =============================================================================

% TODO: What inference engines do
% - Load models into memory
% - Process requests
% - Manage batching
% - Handle streaming
% - Optimize throughput

% =============================================================================
\section{Ollama: Simplicity First}
\label{sec:ollama}
% =============================================================================

% TODO: Ollama deep dive

\subsection{Architecture and Design Philosophy}
\label{subsec:ollama-architecture}

% TODO: How Ollama works
% - Built on llama.cpp
% - Model management abstraction
% - REST API
% - Modelfile system

\subsection{Installation and Configuration}
\label{subsec:ollama-install}

% TODO: Setup instructions
% - All platforms
% - Configuration options
% - Memory management
% - GPU assignment

\subsection{API Reference}
\label{subsec:ollama-api}

% TODO: Key endpoints
% - /api/generate
% - /api/chat
% - /api/embeddings
% - /api/tags
% - /api/show

\begin{programcode}{Ollama API Examples}
\begin{lstlisting}[language=bash]
# Generate completion
curl http://localhost:11434/api/generate -d '{
  "model": "llama3.2:7b",
  "prompt": "Explain quantum computing",
  "stream": false
}'

# Chat completion
curl http://localhost:11434/api/chat -d '{
  "model": "llama3.2:7b",
  "messages": [
    {"role": "user", "content": "Hello!"}
  ]
}'

# Generate embeddings
curl http://localhost:11434/api/embeddings -d '{
  "model": "nomic-embed-text",
  "prompt": "Text to embed"
}'
\end{lstlisting}
\end{programcode}

\subsection{When to Use Ollama}
\label{subsec:ollama-when}

% TODO: Use cases
% - Development and prototyping
% - Single-user deployments
% - Low complexity requirements
% - When model management matters

\begin{svgraybox}
\textbf{Ollama Strengths:}
\begin{itemize}
\item Excellent developer experience
\item Automatic model downloading and management
\item Cross-platform (macOS, Linux, Windows)
\item Good default configurations
\end{itemize}

\textbf{Ollama Limitations:}
\begin{itemize}
\item Limited batching capabilities
\item No continuous batching
\item Single-model focus
\item Less production tuning options
\end{itemize}
\end{svgraybox}

% =============================================================================
\section{llama.cpp: Maximum Flexibility}
\label{sec:llama-cpp}
% =============================================================================

% TODO: llama.cpp deep dive

\subsection{Architecture}
\label{subsec:llama-cpp-architecture}

% TODO: How llama.cpp works
% - C/C++ implementation
% - Multi-backend (CPU, CUDA, Metal, etc.)
% - GGUF format native
% - server binary

\subsection{Building and Installation}
\label{subsec:llama-cpp-install}

% TODO: Compilation options
% - CPU-only build
% - CUDA build
% - Metal build
% - Compilation flags

\subsection{Server Mode}
\label{subsec:llama-cpp-server}

% TODO: Running as a server
% - Command line options
% - API compatibility (OpenAI-like)
% - Configuration parameters

\begin{programcode}{llama.cpp Server Launch}
\begin{lstlisting}[language=bash]
# Basic launch
./llama-server \
  -m models/llama-7b-q4_k_m.gguf \
  --host 0.0.0.0 \
  --port 8080 \
  -c 4096 \
  -ngl 35

# Key parameters:
# -m: Model path
# -c: Context size
# -ngl: Number of GPU layers (for partial offload)
# --host/--port: Network binding
\end{lstlisting}
\end{programcode}

\subsection{Advanced Configuration}
\label{subsec:llama-cpp-advanced}

% TODO: Performance tuning
% - Layer offloading
% - Thread configuration
% - Memory mapping
% - Batch size tuning

\subsection{When to Use llama.cpp}
\label{subsec:llama-cpp-when}

% TODO: Use cases
% - CPU inference
% - Mixed CPU/GPU
% - Custom deployments
% - Resource-constrained environments
% - Edge deployments

% =============================================================================
\section{vLLM: Production Performance}
\label{sec:vllm}
% =============================================================================

% TODO: vLLM deep dive

\subsection{Key Innovation: Paged Attention}
\label{subsec:vllm-paged-attention}

% TODO: Explain paged attention
% - Memory fragmentation problem
% - How paged attention solves it
% - Throughput improvements

\subsection{Continuous Batching}
\label{subsec:vllm-continuous-batching}

% TODO: Dynamic batching explanation
% - Traditional batching limitations
% - How continuous batching works
% - Throughput vs latency trade-offs

\subsection{Installation and Setup}
\label{subsec:vllm-install}

% TODO: Setup instructions
% - pip install
% - Docker deployment
% - GPU requirements

\begin{programcode}{vLLM Server Launch}
\begin{lstlisting}[language=bash]
# Using Python API
python -m vllm.entrypoints.openai.api_server \
  --model meta-llama/Llama-3.2-7b-instruct \
  --dtype float16 \
  --api-key token-abc123 \
  --port 8000

# Docker deployment
docker run --gpus all \
  -v ~/.cache/huggingface:/root/.cache/huggingface \
  -p 8000:8000 \
  vllm/vllm-openai:latest \
  --model meta-llama/Llama-3.2-7b-instruct
\end{lstlisting}
\end{programcode}

\subsection{API and OpenAI Compatibility}
\label{subsec:vllm-api}

% TODO: API reference
% - OpenAI-compatible endpoints
% - /v1/completions
% - /v1/chat/completions
% - /v1/embeddings

\subsection{Performance Tuning}
\label{subsec:vllm-tuning}

% TODO: Optimization parameters
% - tensor-parallel-size
% - max-model-len
% - gpu-memory-utilization
% - dtype options

\subsection{When to Use vLLM}
\label{subsec:vllm-when}

% TODO: Use cases
% - High-throughput production
% - Multiple concurrent users
% - When latency matters
% - Multi-GPU deployments

\begin{svgraybox}
\textbf{vLLM Strengths:}
\begin{itemize}
\item 2-4x higher throughput than naive implementations
\item Continuous batching for efficient GPU utilization
\item OpenAI API compatibility
\item Multi-GPU support (tensor parallelism)
\end{itemize}

\textbf{vLLM Limitations:}
\begin{itemize}
\item GPU-only (no CPU inference)
\item More complex setup
\item Higher memory overhead
\item Limited quantization support (improving)
\end{itemize}
\end{svgraybox}

% =============================================================================
\section{Other Engines Worth Knowing}
\label{sec:other-engines}
% =============================================================================

\subsection{TensorRT-LLM}
\label{subsec:tensorrt-llm}

% TODO: NVIDIA's optimized engine
% - When to use
% - Strengths and limitations

\subsection{Text Generation Inference (TGI)}
\label{subsec:tgi}

% TODO: HuggingFace's engine
% - When to use
% - Comparison to vLLM

\subsection{SGLang}
\label{subsec:sglang}

% TODO: Newer alternative
% - Structured generation
% - Performance claims

% =============================================================================
\section{Engine Comparison and Selection}
\label{sec:engine-comparison}
% =============================================================================

% TODO: Decision framework

\subsection{Comparison Matrix}
\label{subsec:engine-matrix}

% TODO: Comprehensive comparison table
% | Feature | Ollama | llama.cpp | vLLM |
% | Ease of use | Excellent | Good | Moderate |
% | Throughput | Low | Medium | High |
% | CPU support | Yes | Yes | No |
% | Batching | Basic | Basic | Continuous |
% | etc. |

\subsection{Decision Framework}
\label{subsec:engine-decision}

% TODO: How to choose
% - Development: Ollama
% - CPU/Edge: llama.cpp
% - Production GPU: vLLM

% =============================================================================
\section{Hands-On: Running All Three Engines}
\label{sec:hands-on-engines}
% =============================================================================

% TODO: Practical exercise
% - Install and run each engine
% - Same model, same prompts
% - Measure throughput and latency
% - Document differences

% =============================================================================
\section{Summary}
\label{sec:ch04-summary}
% =============================================================================

\begin{important}{Key Takeaways}
\begin{itemize}
\item Ollama excels at simplicity and developer experience
\item llama.cpp provides maximum flexibility, especially for CPU inference
\item vLLM delivers highest throughput for GPU-based production
\item Engine choice depends on your deployment context and requirements
\item All three expose HTTP APIs that your control plane can integrate with
\end{itemize}
\end{important}

% =============================================================================
\section*{Problems}
\addcontentsline{toc}{section}{Problems}
% =============================================================================

\begin{prob}
\label{prob:ch04-throughput-comparison}
\textbf{Throughput Comparison}\\
Using the same 7B model and a set of 100 test prompts, measure the requests-per-minute throughput of Ollama and vLLM. Run with 1, 5, and 10 concurrent clients. Graph and explain your results.
\end{prob}

\begin{prob}
\label{prob:ch04-cpu-vs-gpu}
\textbf{CPU vs GPU Performance}\\
Using llama.cpp, compare inference speed on CPU-only versus GPU with different numbers of layers offloaded. Find the optimal split for your hardware and document the trade-offs.
\end{prob}

\begin{prob}
\label{prob:ch04-engine-abstraction}
\textbf{Engine Abstraction Layer}\\
Design a Go interface that abstracts over different inference engines. Implement two backends: one for Ollama and one for vLLM. Your interface should support both completion and chat modes.
\end{prob}

\input{chapters/references04}
