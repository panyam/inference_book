%%%%%%%%%%%%%%%%%%%%% appendix_a.tex %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% Appendix A: Control Plane Key Abstractions
%
%%%%%%%%%%%%%%%%%%%%%%%% Springer-Verlag %%%%%%%%%%%%%%%%%%%%%%%%%%

\appendix
\chapter{Control Plane Key Abstractions}
\label{ch:appendix-code}

This appendix documents the core interfaces and types that define the control plane architecture. These abstractions remain stable across versions---new features extend them rather than replace them.

For complete implementation code, setup instructions, and deployment templates, see:

\begin{center}
\texttt{github.com/inference-book/inference-plane}
\end{center}

% =============================================================================
\section{Backend Interface}
\label{sec:backend-interface-ref}
% =============================================================================

The \texttt{Backend} interface is the central abstraction for inference engines. Any engine (Ollama, vLLM, TensorRT-LLM) implements this interface to plug into the control plane.

\begin{programcode}{Backend Interface}
\begin{lstlisting}[language=Go]
// Backend represents an inference engine that can generate completions.
type Backend interface {
    // Generate performs inference and returns the response.
    // The context enables timeout and cancellation.
    Generate(ctx context.Context, req GenerateRequest) (GenerateResponse, error)

    // GenerateStream performs streaming inference.
    // Returns a channel that emits response chunks.
    GenerateStream(ctx context.Context, req GenerateRequest) (<-chan StreamChunk, error)

    // Health checks if the backend is operational.
    // Returns nil if healthy, error with details otherwise.
    Health(ctx context.Context) error

    // Name returns the backend identifier for logging and metrics.
    Name() string

    // Models returns the list of available models.
    Models(ctx context.Context) ([]Model, error)
}
\end{lstlisting}
\end{programcode}

% =============================================================================
\section{Request and Response Types}
\label{sec:request-response-ref}
% =============================================================================

Request and response types follow the OpenAI API format for compatibility with existing tools and SDKs.

\begin{programcode}{Generate Request}
\begin{lstlisting}[language=Go]
// GenerateRequest represents a completion request (OpenAI-compatible).
type GenerateRequest struct {
    Model       string    `json:"model"`
    Prompt      string    `json:"prompt,omitempty"`       // For completions
    Messages    []Message `json:"messages,omitempty"`     // For chat
    MaxTokens   int       `json:"max_tokens,omitempty"`
    Temperature float64   `json:"temperature,omitempty"`
    TopP        float64   `json:"top_p,omitempty"`
    Stream      bool      `json:"stream,omitempty"`
    Stop        []string  `json:"stop,omitempty"`
}

// Message represents a chat message.
type Message struct {
    Role    string `json:"role"`    // "system", "user", "assistant"
    Content string `json:"content"`
}
\end{lstlisting}
\end{programcode}

\begin{programcode}{Generate Response}
\begin{lstlisting}[language=Go]
// GenerateResponse represents a completion response (OpenAI-compatible).
type GenerateResponse struct {
    ID      string   `json:"id"`
    Object  string   `json:"object"`
    Created int64    `json:"created"`
    Model   string   `json:"model"`
    Choices []Choice `json:"choices"`
    Usage   Usage    `json:"usage"`
}

// Choice represents a single completion choice.
type Choice struct {
    Index        int     `json:"index"`
    Text         string  `json:"text,omitempty"`          // For completions
    Message      *Message `json:"message,omitempty"`      // For chat
    FinishReason string  `json:"finish_reason"`
}

// Usage tracks token consumption.
type Usage struct {
    PromptTokens     int `json:"prompt_tokens"`
    CompletionTokens int `json:"completion_tokens"`
    TotalTokens      int `json:"total_tokens"`
}
\end{lstlisting}
\end{programcode}

% =============================================================================
\section{Health Check Types}
\label{sec:health-types-ref}
% =============================================================================

Health checking uses a three-state model that enables nuanced operational decisions.

\begin{programcode}{Health Status}
\begin{lstlisting}[language=Go]
// Status represents component health state.
type Status string

const (
    StatusHealthy   Status = "healthy"   // Fully operational
    StatusDegraded  Status = "degraded"  // Working but impaired
    StatusUnhealthy Status = "unhealthy" // Not operational
)

// CheckResult contains health check details.
type CheckResult struct {
    Name      string        `json:"name"`       // Component name
    Status    Status        `json:"status"`     // Health status
    Message   string        `json:"message"`    // Human-readable details
    Latency   time.Duration `json:"latency"`    // Check duration
    Timestamp time.Time     `json:"timestamp"`  // When check ran
}

// HealthResponse aggregates all component health.
type HealthResponse struct {
    Status  Status        `json:"status"`   // Overall status
    Checks  []CheckResult `json:"checks"`   // Individual components
}
\end{lstlisting}
\end{programcode}

% =============================================================================
\section{Configuration}
\label{sec:config-ref}
% =============================================================================

Configuration supports YAML files with environment variable overrides.

\begin{programcode}{Configuration Structure}
\begin{lstlisting}[language=Go]
// Config holds all control plane configuration.
type Config struct {
    Server   ServerConfig   `yaml:"server"`
    Backend  BackendConfig  `yaml:"backend"`
    Metrics  MetricsConfig  `yaml:"metrics"`
    Logging  LoggingConfig  `yaml:"logging"`
    Auth     AuthConfig     `yaml:"auth"`      // v0.2+
    Cache    CacheConfig    `yaml:"cache"`     // v0.2+
    RateLimit RateLimitConfig `yaml:"rate_limit"` // v0.2+
}

// ServerConfig defines HTTP server settings.
type ServerConfig struct {
    Host         string        `yaml:"host" env:"CP_HOST"`
    Port         int           `yaml:"port" env:"CP_PORT"`
    ReadTimeout  time.Duration `yaml:"read_timeout"`
    WriteTimeout time.Duration `yaml:"write_timeout"`
}

// BackendConfig defines inference backend settings.
type BackendConfig struct {
    Type    string        `yaml:"type"`    // "ollama", "vllm", etc.
    URL     string        `yaml:"url" env:"CP_BACKEND_URL"`
    Timeout time.Duration `yaml:"timeout"`
}
\end{lstlisting}
\end{programcode}

% =============================================================================
\section{Observability with OpenTelemetry}
\label{sec:otel-ref}
% =============================================================================

The control plane uses OpenTelemetry for unified observability---metrics, traces, and logs through a single SDK. OpenTelemetry is vendor-neutral: export to Prometheus, Jaeger, Grafana, Datadog, or any OTLP-compatible backend.

\subsection{Telemetry Provider Setup}
\label{subsec:otel-setup}

\begin{programcode}{OpenTelemetry Initialization}
\begin{lstlisting}[language=Go]
// InitTelemetry sets up OpenTelemetry with OTLP export.
func InitTelemetry(ctx context.Context, serviceName string) (func(), error) {
    // Resource identifies this service in telemetry backends
    res, err := resource.New(ctx,
        resource.WithAttributes(
            semconv.ServiceName(serviceName),
            semconv.ServiceVersion("0.1.0"),
        ),
    )
    if err != nil {
        return nil, err
    }

    // Metrics: OTLP exporter (works with Prometheus, Grafana, etc.)
    metricExporter, err := otlpmetricgrpc.New(ctx)
    if err != nil {
        return nil, err
    }
    meterProvider := metric.NewMeterProvider(
        metric.WithResource(res),
        metric.WithReader(metric.NewPeriodicReader(metricExporter)),
    )
    otel.SetMeterProvider(meterProvider)

    // Traces: OTLP exporter (works with Jaeger, Tempo, etc.)
    traceExporter, err := otlptracegrpc.New(ctx)
    if err != nil {
        return nil, err
    }
    tracerProvider := trace.NewTracerProvider(
        trace.WithResource(res),
        trace.WithBatcher(traceExporter),
    )
    otel.SetTracerProvider(tracerProvider)

    // Cleanup function for graceful shutdown
    return func() {
        meterProvider.Shutdown(ctx)
        tracerProvider.Shutdown(ctx)
    }, nil
}
\end{lstlisting}
\end{programcode}

\subsection{Metrics}
\label{subsec:metrics-ref}

\begin{programcode}{Core Metrics with OpenTelemetry}
\begin{lstlisting}[language=Go]
// Metrics holds OpenTelemetry metric instruments.
type Metrics struct {
    RequestsTotal   metric.Int64Counter
    RequestDuration metric.Float64Histogram
    TokensGenerated metric.Int64Counter
    BackendHealthy  metric.Int64UpDownCounter
}

// NewMetrics creates metric instruments from the global meter provider.
func NewMetrics() (*Metrics, error) {
    meter := otel.Meter("inference-control-plane")

    requestsTotal, err := meter.Int64Counter(
        "inference.requests.total",
        metric.WithDescription("Total inference requests"),
        metric.WithUnit("{request}"),
    )
    if err != nil {
        return nil, err
    }

    requestDuration, err := meter.Float64Histogram(
        "inference.request.duration",
        metric.WithDescription("Request duration in seconds"),
        metric.WithUnit("s"),
        metric.WithExplicitBucketBoundaries(0.1, 0.5, 1, 2, 5, 10, 30, 60),
    )
    if err != nil {
        return nil, err
    }

    tokensGenerated, err := meter.Int64Counter(
        "inference.tokens.generated",
        metric.WithDescription("Total tokens generated"),
        metric.WithUnit("{token}"),
    )
    if err != nil {
        return nil, err
    }

    backendHealthy, err := meter.Int64UpDownCounter(
        "inference.backend.healthy",
        metric.WithDescription("Backend health (1=healthy, 0=unhealthy)"),
    )
    if err != nil {
        return nil, err
    }

    return &Metrics{
        RequestsTotal:   requestsTotal,
        RequestDuration: requestDuration,
        TokensGenerated: tokensGenerated,
        BackendHealthy:  backendHealthy,
    }, nil
}
\end{lstlisting}
\end{programcode}

\subsection{Distributed Tracing}
\label{subsec:tracing-ref}

\begin{programcode}{Request Tracing}
\begin{lstlisting}[language=Go]
// TraceMiddleware adds distributed tracing to requests.
func TraceMiddleware(next http.Handler) http.Handler {
    return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
        tracer := otel.Tracer("inference-control-plane")
        ctx, span := tracer.Start(r.Context(), r.URL.Path,
            trace.WithAttributes(
                attribute.String("http.method", r.Method),
                attribute.String("http.url", r.URL.String()),
            ),
        )
        defer span.End()

        // Pass trace context to backend calls
        next.ServeHTTP(w, r.WithContext(ctx))
    })
}

// PropagateTrace injects trace context into outgoing HTTP requests.
func PropagateTrace(ctx context.Context, req *http.Request) {
    otel.GetTextMapPropagator().Inject(ctx, propagation.HeaderCarrier(req.Header))
}
\end{lstlisting}
\end{programcode}

% =============================================================================
\section{Middleware Chain}
\label{sec:middleware-ref}
% =============================================================================

Requests pass through a middleware chain that handles cross-cutting concerns.

\begin{programcode}{Middleware Interface}
\begin{lstlisting}[language=Go]
// Middleware wraps an HTTP handler with additional behavior.
type Middleware func(http.Handler) http.Handler

// Chain composes multiple middlewares into a single middleware.
func Chain(middlewares ...Middleware) Middleware {
    return func(next http.Handler) http.Handler {
        for i := len(middlewares) - 1; i >= 0; i-- {
            next = middlewares[i](next)
        }
        return next
    }
}

// Standard middleware chain (order matters):
// 1. Recovery   - Catches panics, returns 500
// 2. RequestID  - Assigns unique ID for tracing
// 3. Logging    - Logs request/response details
// 4. Metrics    - Records latency and counts
// 5. Auth       - Validates credentials (v0.2+)
// 6. RateLimit  - Enforces rate limits (v0.2+)
\end{lstlisting}
\end{programcode}

% =============================================================================
\section{Error Types}
\label{sec:errors-ref}
% =============================================================================

Structured errors enable consistent error handling and client responses.

\begin{programcode}{Error Types}
\begin{lstlisting}[language=Go]
// APIError represents a structured error response.
type APIError struct {
    Code    string `json:"code"`
    Message string `json:"message"`
    Details any    `json:"details,omitempty"`
}

// Common error codes
const (
    ErrCodeInvalidRequest  = "invalid_request"
    ErrCodeAuthentication  = "authentication_error"
    ErrCodeRateLimited     = "rate_limit_exceeded"
    ErrCodeBackendError    = "backend_error"
    ErrCodeBackendTimeout  = "backend_timeout"
    ErrCodeModelNotFound   = "model_not_found"
    ErrCodeInternalError   = "internal_error"
)

// Error implements the error interface.
func (e APIError) Error() string {
    return e.Message
}
\end{lstlisting}
\end{programcode}

% =============================================================================
\section{Version Evolution}
\label{sec:version-evolution}
% =============================================================================

The interfaces above form the stable core. Each version adds capabilities:

\begin{center}
\begin{tabular}{lll}
\toprule
\textbf{Version} & \textbf{Chapter} & \textbf{Additions} \\
\midrule
v0.1 & 6 & Backend, Health, Metrics, Logging \\
v0.2 & 7-9 & Auth, RateLimit, Cache interfaces \\
v0.3 & 12-15 & Tenant, Billing, Router interfaces \\
v1.0 & 18-19 & Cluster, Scheduler, Autoscaler interfaces \\
\bottomrule
\end{tabular}
\end{center}

Each version's complete implementation is available as a git tag:

\begin{lstlisting}[language=bash]
git checkout v0.1  # Chapter 6 implementation
git checkout v0.2  # After Part II
git checkout v0.3  # After Part III
git checkout v1.0  # Complete platform
\end{lstlisting}
